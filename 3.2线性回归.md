```python
%matplotlib inline
import random
import torch
from d2l import torch as d2l
```


```python
def synthetic_data(w,b,num_examples):#@save
    """生成y = Xw + b + 噪声。"""
    X = torch.normal(0,1,(num_examples,len(w)))#w是一个含有两个元素的向量2*1，X是1000*2
    y = torch.matmul(X,w) + b            #matmul(),是矩阵乘法，矩阵乘以向量得到一个向量1000*2*2*1=1000*1
    y += torch.normal(0,0.01,y.shape)    #这一步加入了噪声项，均值为0，方差为0.01
    return X,y.reshape((-1,1))           #让他们以一个列向量返回，（-1，1）会自动生成一列

true_w = torch.tensor([2,-3.4])
true_b = 4.2
features,labels = synthetic_data(true_w,true_b,1000)

```


```python
print('features:',features[0],'\nlabel:',labels[0])
print(features)
```

    features: tensor([ 0.2070, -0.3918]) 
    label: tensor([5.9455])
    tensor([[ 0.2070, -0.3918],
            [ 0.3070,  0.0915],
            [-0.4879, -0.0251],
            ...,
            [-1.4649,  0.0872],
            [-0.5443, -0.8888],
            [-0.7503, -1.4747]])
    


```python
d2l.set_figsize()
d2l.plt.scatter(features[:,1].detach().numpy(),labels.detach().numpy(),1);
#绘制以第 2 列特征为 x 轴，标签值为 y 轴的散点图，展示单个特征与标签之间的线性关系
#由于我们生成的数据遵循y = 2x₁ - 3.4x₂ + 4.2 + 噪声的线性关系，这个散点图会呈现出明显的线性趋势
#反映第 2 个特征（x₂）与标签 y 之间的负相关关系（因为系数是 - 3.4）
```


    
![svg](output_3_0.svg)
    



```python
def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    #这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices[i: min(i + batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]#tensor张量能用作下标

batch_size = 10
for X,y in data_iter(batch_size,features,labels):
    print(X,'\n',y)
    break
```

    tensor([[ 0.0694, -0.2318],
            [-1.1805, -1.6517],
            [-0.8583, -0.3226],
            [-0.1393, -0.7517],
            [-0.1084,  0.0700],
            [ 0.1843,  0.5811],
            [-0.9251, -0.7199],
            [ 0.6833, -0.0925],
            [-0.2474, -0.1436],
            [ 1.3513,  0.5565]]) 
     tensor([[5.1418],
            [7.4506],
            [3.5797],
            [6.4759],
            [3.7602],
            [2.5747],
            [4.7972],
            [5.8749],
            [4.1946],
            [5.0212]])
    


```python
#初始化模型参数
w = torch.normal(0,0.01,size=(2,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True)
```


```python
#定义模型
def linreg(X,w,b):#@save
    """线性回归模型."""
    return torch.matmul(X,w)+ b
```


```python
#定义损失函数
def squared_loss(y_hat,y):#@save
    """均方损失"""#在代码中，由于无法直接输入数学符号 “y^”，所以用下划线加 hat 的形式 y_hat 来表示，这是学术界和工业界的通用写法。
    return (y_hat - y.reshape(y_hat.shape))**2/2
```


```python
#定义优化算法
def sgd(params,lr,batch_size):#@save
    """小批量随机梯度下降。"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad/batch_size
            param.grad.zero_()
```


```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X,y in data_iter(batch_size,features,labels):
        l = loss(net(X,w,b),y)#X和y 的小批量损失
        #因为l的形状是(batch_size,1)而不是一个标量
        l.sum().backward()
        #l中的所有元素被加到一起，并以此计算关于[w,b]的梯度
        sgd([w,b],lr,batch_size)#使用参数的梯度更新参数，# 参数更新时除以batch_size，等价于用平均损失更新
    with torch.no_grad():
        train_l = loss(net(features,w,b),labels)
        print(f'epoch{epoch + 1},loss{float(train_l.mean()):f}')
```

    epoch1,loss0.036294
    epoch2,loss0.000131
    epoch3,loss0.000051
    


```python
print(f'w的估计误差:{true_w-w.reshape(true_w.shape)}')
print(f'b的估计误差:{true_b-b}')
```

    w的估计误差:tensor([ 0.0002, -0.0008], grad_fn=<SubBackward0>)
    b的估计误差:tensor([0.0006], grad_fn=<RsubBackward1>)
    


```python

```


```python

```
