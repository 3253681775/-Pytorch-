```python
import torch
from torch import nn
from d2l import torch as d2l
```


```python
def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):
    #momentum	浮点数 (float)	移动平均的动量系数，控制历史值与当前值的融合比例
    #通过is_grad_enables方法来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        #如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X-moving_mean)/torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2,4)
        #assert断言：限制输入X的形状只能是 2 维（全连接层，(N, C)）或 4 维（卷积层，(N, C, H, W)），这是 BN 层最常见的两种输入场景
        if len(X.shape) == 2:
            #使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            #X.mean(dim=0)：沿 ** 批次维度（dim=0）** 计算均值，得到形状为(C,)的均值向量，代表每个特征维度在当前批次的均值；
            var = ((X-mean)**2).mean(dim=0)
        else:
            #使用二维卷积层的情况，计算通道维上（axis=1)的均值和方差
            #这里我们需要保持X的形状以便后面可以做广播运算
            #BN 在卷积层的核心规则：沿通道维度归一化，即对每个通道，计算该通道所有批次、所有空间位置的均值 / 方差；
            mean = X.mean(dim = (0,2,3),keepdim=True)#沿谁谁化作1
            var = ((X-mean)**2).mean(dim=(0,2,3),keepdim=True)
            X.mean(dim=(0,2,3), keepdim=True)
# dim=(0,2,3)：沿 ** 批次（0）、高度（2）、宽度（3）** 维度求平均，仅保留通道维度（1）；
# keepdim=True：保持输出的维度为(1, C, 1, 1)，而非压缩为(C,)，目的是为了后续与原始X做广播运算（形状匹配）；
        #训练模式下，用当前的均和方差做标准化
        X_hat = (X-mean)/torch.sqrt(var+eps)
        #更新移动平均的均值和方差
        moving_mean = momentum*moving_mean + (1.0 - momentum)*mean
        moving_var = momentum*moving_var + (1.0 - momentum)*var
        #核心作用：训练过程中，通过指数移动平均逐步融合当前批次的均值 / 方差和历史的移动均值 / 方差，得到全局统计量，用于预测模式；
        #momentum 的意义：通常取 0.9/0.99/0.999，值越大，越重视历史的移动均值 / 方差，统计量越稳定。
    Y = gamma * X_hat + beta #缩放和位移
    #标准化后的特征均值为 0、方差为 1，可能会丢失原有的特征表达能力，
    #因此引入 ** 可学习的缩放（\(\gamma\)）和位移（\(\beta\)）** 
    #参数恢复特征信息；变换公式：\(Y = \gamma \times \hat{X} + \beta\)，
    #其中：若\(\gamma = \sqrt{\sigma^2_{batch}}\)、\(\beta = \mu_{batch}\)，则Y还原为原始X；
    #模型通过反向传播学习\(\gamma\)和\(\beta\)，自适应调整特征的分布
    return Y,moving_mean.data,moving_var.data
```


```python
class BatchNorm(nn.Module):
    #num_features:全连接层的输出数量或卷积层的输出通道数
    #num_dims:2表示完全连接层，4表示卷积层
    def __init__(self,num_features,num_dims):
        super().__init__()
        if num_dims ==2:
            shape = (1,num_features)
        else:
            shape = (1,num_features,1,1)
        #核心目的：根据输入维度（全连接 / 卷积），确定可学习参数（\(\gamma\)、\(\beta\)）和非学习参数（移动均值 / 方差）的形状，保证后续与输入特征做广播运算。
        # 分支逻辑：num_dims == 2（全连接层）：输入形状为(N, C)（N= 批次，C= 特征数），参数形状设为(1, num_features)，可与输入按批次维度广播。num_dims != 2（卷积层）：输入形状为(N, C, H, W)，参数形状设为(1, num_features, 1, 1)，可与输入按批次、高度、宽度维度广播。设计考量：
        #广播运算能让单个通道的参数作用于该通道的所有空间位置 / 批次样本，符合 BN “按通道归一化” 的核心规则。
        
        #参与求梯度和迭代的拉伸参数和偏移参数，其分别初始化成1和0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
#     nn.Parameter：PyTorch 的核心类，将普通张量包装为模型可学习参数，具备以下特性：
# 会被model.parameters()自动收集，参与优化器的参数更新。
# 会自动跟踪梯度（requires_grad=True），支持反向传播。
# 随模型一起迁移到指定设备（如 GPU）。
        
        #非模型参数的变量初始化为0和1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)
        
    def forward(self,X):
        #如果X不在内存上，将moving_mean和moving_var
        #复制到X所在的显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
            #保存更新过的moving_mean和moving_var
            #moving_mean/moving_var是初始化在CPU的普通张量，而输入X可能在GPU上（模型训练时通常用 GPU），若设备不一致会导致张量运算报错。
            # 逻辑解析：
            # self.moving_mean.device：获取移动均值当前所在的设备（CPU/GPU）。
            # X.device：获取输入特征所在的设备。
            # 若设备不一致，调用to(X.device)将moving_mean/moving_var迁移到X的设备上。
        Y,self.moving_mean,self.moving_var = batch_norm(
            X,self.gamma,self.beta,self.moving_mean,
            self.moving_var,eps=1e-5,momentum=0.9)
        return Y
```


```python
#使用批量规范化层的LeNet
net = nn.Sequential(
    nn.Conv2d(1,6,kernel_size=5),BatchNorm(6,num_dims=4),nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2,stride=2),
    nn.Conv2d(6,16,kernel_size=5),BatchNorm(16,num_dims=4),nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2,stride=2),nn.Flatten(),
    nn.Linear(16*4*4,120),BatchNorm(120,num_dims=2),nn.Sigmoid(),
    nn.Linear(120,84),BatchNorm(84,num_dims=2),nn.Sigmoid(),
    nn.Linear(84,10))
```


```python

```
