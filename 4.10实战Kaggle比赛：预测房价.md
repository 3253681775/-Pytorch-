```python
import hashlib# 用于计算文件哈希值，验证文件完整性
import os# 用于文件路径和目录操作
import tarfile# 用于处理tar压缩文件
import zipfile# 用于处理zip压缩文件
import requests # 用于发送HTTP请求下载文件

#@save
DATA_HUB = dict() # 创建一个字典，用于存储数据集名称与对应的URL和哈希值
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'# 数据集的基础URL
#URL 就是统一资源定位符，简单说就是我们常说的 “网址”，用来在互联网上精准找到某个资源（比如网页、图片、文件）的地址

#'http://d2l-data.s3-accelerate.amazonaws.com/'它本身不是一个可直接打开的 “网页”，如果你把它复制到浏览器地址栏，会显示 “访问被拒绝”（因为它只用于程序下载文件，不提供网页浏览功能）。
#它的核心作用是作为 “数据集仓库的大门”，配合具体的文件路径（比如 /kaggle_house_pred_train.csv），
#就能让代码里的 requests 库精准找到并下载所需的数据集文件。
```


```python
#用来下载数据集，将数据集缓存在本都目录中，并返回下载文件的名称
def download(name,cache_dir=os.path.join('..','data')):#@save
    #name：要下载的数据集名称（必须在 DATA_HUB 字典中注册过）。
    #cache_dir：本地缓存目录，默认是当前目录的上一级 data 文件夹（../data）
    """下载一个DATA_HUB中的文件，返回本地文件名"""
    assert name in DATA_HUB,f"{name}不存在于{DATA_HUB}"
    #assert 是断言语句，用于检查前置条件：如果 name 不在 DATA_HUB 中，会直接报错并提示信息。
    
    url,shal_hash = DATA_HUB[name]
    #url：文件的网络下载地址（如 http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv）。
    #sha1_hash：文件的 SHA1 哈希值（用于校验文件完整性）
    
    os.makedirs(cache_dir,exist_ok=True)
    #创建文件夹（支持多级目录）
    #exist_ok=True：如果目录已存在，不会报错（避免重复创建的错误）
    
    fname = os.path.join(cache_dir,url.split('/')[-1])
    #url.split('/')[-1]：从 URL 中提取文件名。例如，url 为 xxx/kaggle_house_pred_train.csv 时，提取结果是 kaggle_house_pred_train.csv
    #os.path.join：将缓存目录和文件名拼接成完整的本地路径（如 ../data/kaggle_house_pred_train.csv）
    
    if os.path.exists(fname):
        #：检查文件是否已下载到本地。
        sha1 = hashlib.sha1()#创建一个 SHA1 哈希对象（用于计算文件哈希）
        with open(fname,'rb') as f:#以二进制模式打开文件（确保哈希计算准确）
            while True:
                data = f.read(1048576)#每次读取 1MB 数据（避免一次性读取大文件占用过多内存）
                if not data:
                    break
                sha1.update(data)#用读取的部分数据更新哈希值。
                #这里的 “更新” 不是覆盖之前的结果，而是把新读的data内容融入到已有的计算中。
                #比如第一次读 1MB，计算器算这 1MB 的临时哈希；第二次再读 1MB，计算器会结合第一次的结果，算前 2MB 的临时哈希，直到读完所有内容
        if sha1.hexdigest() == shal_hash:
            #生成最终的哈希字符串（与 DATA_HUB 中存储的 sha1_hash 对比）
            #final_hash = sha1.hexdigest()  # 把最终计算结果转成十六进制字符串
            return fname #命中缓存，#若哈希值匹配，说明本地文件完整且未被修改，直接返回文件路径（无需重新下载）
    
    print(f'正在从{url}下载{fname}...')
    r = requests.get(url,stream=True,verify=True)
    #发送 HTTP 请求获取文件，stream=True 表示流式下载（适合大文件）。
    #verify=True：验证 SSL 证书（确保连接安全）
    
    with open(fname,'wb') as f:
        #以二进制写模式打开本地文件
        f.write(r.content)
        #将下载的内容写入本地文件（完成保存）
    return fname
```


```python
#一个将下载并解压缩一个zip或者ter文件，一个是将本书中使用的所有数据集从DATA_HUB下载到缓存目录中
def download_extract(name,folder=None):#@save
    #name：要下载和解压的数据集名称（需在 DATA_HUB 中注册）
    #folder：可选参数，指定解压后文件的存放文件夹，默认 None
    """下载并解压zio/tar文件"""
    
    fname = download(name)
    #调用前面定义的 download 函数下载文件，返回本地文件路径并赋值给 fname
    #这一步确保文件已下载到本地（如果有缓存会直接使用缓存）
    
    base_dir = os.path.dirname(fname)
    #os.path.dirname(fname) 获取下载文件所在的目录路径
    #例如：如果 fname 是 ../data/test.zip，则 base_dir 是 ../data
    
    data_dir,ext = os.path.splitext(fname)
    # 拆分文件路径为文件名和扩展名
    #例如：../data/test.zip 会拆分为 ('../data/test', '.zip')
    #data_dir 接收文件名部分，ext 接收扩展名部分
    
    if ext == '.zip':
        fp = zipfile.ZipFile(fname,'r')
        #判断如果文件扩展名是 .zip，则使用 zipfile.ZipFile 打开文件
        #'r' 表示以只读模式打开压缩文件
        
    elif ext in ('.tar','.gz'):
        fp = tarfile.open(fname,'r')
        #如果扩展名是 .tar 或 .gz（常见的 tar 压缩格式），则使用 tarfile.open 打开文件
        #同样以只读模式打开
        
    else:
        assert False,'只有zip/tar文件可以被解压缩'
        #如果是其他格式的文件，通过 assert 语句抛出错误，提示不支持的文件格式
        #assert False 会直接触发 AssertionError 异常
    fp.extractall(base_dir)
    #将压缩文件中的所有内容解压到 base_dir 目录（即下载文件所在的目录）
    #这一步会自动处理压缩包内的文件和文件夹结构
    
    return os.path.join(base_dir,folder) if folder else data_dir
    #根据是否指定 folder 参数返回不同的路径：
    #如果指定了 folder，则返回 base_dir + folder 的组合路径
    #如果未指定 folder，则返回 data_dir（即去掉扩展名的下载文件路径）
    #作用是返回解压后文件的根目录路径，方便后续读取数据
    
def download_all():#@save
    """下载DATA_HUB中的所有文件"""
    for name in DATA_HUB:
        download(name)
```


```python
%matplotlib inline
import numpy as np
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l
```


```python
DATA_HUB['kaggle_house_train'] = (#@save
    DATA_URL + 'kaggle_house_pred_train.csv','585e9cc93e70b39160e7921475f9bcd7d31219ce')
#DATA_HUB：之前定义的空字典，用于存储 “数据集名称→（下载 URL，哈希值）” 的映射关系，相当于一个 “数据集注册表”。
#'kaggle_house_train'：这是训练集的唯一标识名称（键），后续调用download('kaggle_house_train')时，就是通过这个名称找到对应的下载信息。
#=：赋值操作，将右边的 “（URL，哈希值）元组” 绑定到左边的键上。

#第一个元素：DATA_URL + 'kaggle_house_pred_train.csv'
#DATA_URL：之前定义的基础地址http://d2l-data.s3-accelerate.amazonaws.com/。
#拼接后得到训练集的完整下载 URL：http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv。
#作用：告诉download函数 “要从这个网络地址下载训练集文件”。

#第二个元素：'585e9cc93e70b39160e7921475f9bcd7d31219ce'
#这是训练集文件的SHA1 哈希值（一串 40 位的十六进制字符串）。
#作用：用于download函数校验文件完整性 —— 下载后计算本地文件的 SHA1 值，与这个值对比，如果一致说明文件没损坏、没被篡改；如果不一致则需要重新下载。

DATA_HUB['kaggle_house_test'] = (#@save
    DATA_URL + 'kaggle_house_pred_test.csv','fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

```


```python
train_data = pd.read_csv(download('kaggle_house_train'))
test_data = pd.read_csv(download('kaggle_house_test'))
```


```python
print(train_data.shape)
print(test_data.shape)
```

    (1460, 81)
    (1459, 80)
    


```python
print(train_data.iloc[0:4,[0,1,2,3,-3,-2,-1]])
```

       Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice
    0   1          60       RL         65.0       WD        Normal     208500
    1   2          20       RL         80.0       WD        Normal     181500
    2   3          60       RL         68.0       WD        Normal     223500
    3   4          70       RL         60.0       WD       Abnorml     140000
    


```python
all_features = pd.concat((train_data.iloc[:,1:-1],test_data.iloc[:,1:]))
#pd.concat() 是 pandas 中用于拼接数据的函数，它可以沿着指定轴（行或列）将多个 DataFrame 或 Series 合并成一个。
#这里的参数是一个元组 (train_data 的特征, test_data 的特征)，表示要拼接这两部分数据。

#train_data 是训练集的 DataFrame，包含特征列和标签列（房价）。
#iloc[:,1:-1] 是 pandas 的位置索引器，用于按位置选取数据：
#第一个 : 表示选取所有行（保留所有样本）。
#1:-1 表示选取从第 2 列到倒数第 2 列（排除第 1 列和最后 1 列）：
#排除第 1 列：通常是样本 ID（如 Id 列），不参与模型训练。
#排除最后 1 列：训练集的标签列（如 SalePrice，即房价），这是我们要预测的目标，不能混入特征中。

#test_data 是测试集的 DataFrame，包含特征列，但没有标签列（因为是待预测的数据）。
#iloc[:,1:] 表示选取所有行和从第 2 列到最后一列：
#排除第 1 列：同样是样本 ID（Id 列），不参与模型训练。
#保留其余所有列：都是测试集的特征，需要和训练集的特征做相同处理

#经过 pd.concat() 拼接后，all_features 成为一个包含训练集和测试集所有特征的 DataFrame。
# 拼接方式：按行拼接（默认 axis=0），即训练集的所有样本在前，测试集的所有样本在后。
# 行数 = 训练集样本数 + 测试集样本数。
# 列数 = 特征列数（训练集和测试集的特征列完全一致，否则拼接会报错）。
```


```python
#若无法获得测试数据，可根据训练数据计算均值和标准差

numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
# all_features 是一个 DataFrame（表格型数据结构），而 .dtypes 是 DataFrame 的一个属性，用于返回每一列的数据类型，结果是一个 Series（一维数组）：
# 索引（index）：是原 DataFrame 的列名（如 'Id'、'MSSubClass' 等特征名）。
# 值（values）：是对应列的数据类型（如 int64、float64 表示数值型，object 表示文本 / 类别型，datetime64 表示日期型等）。

# 第二步：all_features.dtypes != 'object' 的作用
# 这是一个布尔运算，用 != 对 dtypes 返回的 Series 进行逐元素判断：
# 对于每个元素（即每列的数据类型），如果不是 'object' 类型（即数值型、日期型等），返回 True；
# 如果是 'object' 类型（文本 / 类别型），返回 False。
#得到的是一个布尔型 Series，其索引仍是原列名，值为 True/False

# 第三步：用布尔 Series 筛选并取索引
# all_features.dtypes[all_features.dtypes != 'object'] 是布尔索引操作：
# pandas 允许用一个布尔型 Series 作为索引，筛选出值为 True 的元素

#最后通过 .index 获取这个筛选后 Series 的索引，即所有数值型特征的列名，存入 numeric_features
#numeric_features 的结构是 pandas 的 Index 对象，本质上是一个存储 “数值型特征列名” 的有序集合，类似于一个特殊的列表（但具备更多 pandas 特有的功能）
#可迭代：可以用 for 循环遍历每个数值型特征的列名，支持索引：可以用位置索引访问特定列名，与 DataFrame 兼容：可以直接作为索引用于 DataFrame 筛选

all_features[numeric_features] = all_features[numeric_features].apply(lambda x:(x-x.mean())/(x.std()))
#目的：将数值特征标准化为 “均值为 0，标准差为 1” 的分布，消除不同特征的量纲影响（如 “面积” 单位是平方米，“房间数” 是个数，直接比较数值没有意义）。
#在标准化数据之后，所有的均值消失，因此我们可以将缺失值设置为0
# all_features[numeric_features]：选取所有数值型特征列。
# .apply(lambda x: ...)：对每一列（x 代表一列数据）应用标准化公式。
# 标准化公式：(x - x.mean()) / x.std()# x.mean()：该列的平均值。# x.std()：该列的标准差（衡量数据离散程度）

all_features[numeric_features] = all_features[numeric_features].fillna(0)
# .fillna(0)：将列中的所有缺失值替换为 0。
# 为什么可以填 0？因为上一步标准化后，所有特征的均值已经是 0（(x-x.mean()) 会让均值变为 0），用 0 填充缺失值不会引入新的偏差，相当于 “用该特征的平均水平填补缺失”。
# 替代方案：如果不做标准化，通常会用该列的均值或中位数填充，但标准化后填 0 是最简便且合理的选择。
```


```python
#ummy_na = True将na(缺失值)视为有效地特征值，并为其创建指示符特征

all_features = pd.get_dummies(all_features,dummy_na=True)
#1. 独热编码：pd.get_dummies(all_features, dummy_na=True)
#pd.get_dummies() 是 pandas 中处理类别型特征的核心函数，作用是将文本 / 离散型特征（如 “房屋类型”“颜色” 等）转换为机器学习模型可识别的数值型特征。
#pd.get_dummies() 会将该列转换为 新特征（原特征会被删除）

# 关键参数：dummy_na=True
# 这是一个非常重要的参数，它指定：将缺失值（NaN）视为一种独立的有效类别，并为其创建专门的指示特征（即 “是否为缺失值” 的标记）。
# 如果不设置（默认 dummy_na=False），缺失值会被忽略，不会为其创建特征，可能导致信息丢失。

all_features.shape
```




    (2919, 331)




```python
n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values,dtype=torch.float32)
# 目的：从合并的 all_features 中拆分出训练集特征，并转换为 PyTorch 张量。
#拆解逻辑：
# all_features[:n_train]：通过切片选取 all_features 的前 n_train 行（因为 all_features 是按 “训练集在前、测试集在后” 拼接的，前 n_train 行正好是训练集特征）。
# .values：将 DataFrame 格式转换为NumPy 数组（shape 为 (n_train, 特征数)），这是从 pandas 到 PyTorch 的中间过渡格式。
# torch.tensor(..., dtype=torch.float32)：将 NumPy 数组转换为 PyTorch 张量（Tensor），并指定数据类型为 32 位浮点数（float32，深度学习中常用的数值类型，平衡精度和计算效率）。
# 结果：train_features 是形状为 (n_train, 特征数) 的张量，包含所有训练样本的预处理后特征，可直接输入模型。

test_features = torch.tensor(all_features[n_train:].values,dtype=torch.float32)
train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1,1),dtype=torch.float32)
```


```python
#训练
loss = nn.MSELoss()
in_features = train_features.shape[1]

def get_net():
    net = nn.Sequential(nn.Linear(in_features,1))
    #第一个参数 in_features：输入特征维度（必须与前面计算的特征数一致）。
    #第二个参数 1：输出维度（这里是 1，因为房价预测是单值输出）。
    return net
```


```python
def log_rmse(net,features,labels):
    #为了在取对数时进一步稳定该值，将小于1的值设置为1
    clipped_preds = torch.clamp(net(features),1,float('inf'))
    #input：需要处理的张量（输入数据）。
    # min：最小值边界（可选）。所有小于 min 的元素会被强制改为 min。
    # max：最大值边界（可选）。所有大于 max 的元素会被强制改为 max。
    # 返回值：一个新的张量，其中元素值都在 [min, max] 范围内（原张量不变）
    
    rmse = torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels)))
    #rmse 是一个 PyTorch 张量（即使只有一个值，也保持张量类型）
    
    return rmse.item()
    #.item() 方法将张量中的单个数值提取为 Python 标量（如 3.21），方便后续打印、存储或比较。
```


```python
def train(net,train_features,train_labels,test_features,test_labels,num_epochs,
          learning_rate,weight_decay,batch_size):
    train_ls,test_ls = [],[]
    train_iter = d2l.load_array((train_features,train_labels),batch_size)
    #d2l.load_array 是 D2L 库中的工具函数，作用是将训练特征和标签打包成批量迭代器。
    # 功能：每次迭代从训练集中随机抽取 batch_size 个样本（形成一个 “批次”），
    # 用于模型的小批量梯度下降（比全量梯度下降更高效，收敛更快）。
    #这里使用的是Adam优化器
    
    optimizer = torch.optim.Adam(net.parameters(),lr = learning_rate,
                                 weight_decay = weight_decay)
    #在 PyTorch 中，weight_decay 默认对应L2 正则化，它会在损失函数中增加一项 “参数平方和” 的惩罚项
    #net.parameters()：模型中需要优化的参数（如线性层的权重 w 和偏置 b）。
    #weight_decay=weight_decay：权重衰减（L2 正则化），通过对参数施加惩罚，防止参数过大导致过拟合
    #\(\lambda\)：即 weight_decay 的取值（超参数，由用户设定，如 0.001、0.01），
    #控制惩罚力度；\(\sum_{w} w^2\)：对模型所有可训练权重 w 的平方求和（偏置 b 通常不惩罚，或惩罚力度更小）。
    for epoch in range(num_epochs):
        for X,y in train_iter:
            optimizer.zero_grad()
            #清除优化器中所有参数的历史梯度。因为 PyTorch 的梯度会累积（方便梯度下降的某些变种）
            #所以每次迭代前必须清零，避免前一次的梯度干扰当前计算。
            l = loss(net(X),y)
            l.backward()
            optimizer.step()
        train_ls.append(log_rmse(net,train_features,train_labels))
        if test_labels is not None:
            test_ls.append(log_rmse(net,test_features,test_labels))
    return train_ls,test_ls
```


```python
def get_k_fold_data(k,i,X,y):
    #k：K 折交叉验证的 “K 值”（如 5 折、10 折，需大于 1）。
    # i：当前折数的索引（0 到 k-1，指定哪一份作为验证集）。
    # X：所有样本的特征数据（张量，形状为 (总样本数, 特征数)）。
    # y：所有样本的标签数据（张量，形状为 (总样本数, 1)）
    
    assert k > 1
    #assert 断言确保 k 大于 1（至少 2 折才叫交叉验证），若不满足则直接报错，避免无效的参数输入。
    
    fold_size = X.shape[0]//k
    #X.shape[0] 是总样本数，// k 表示对总样本数做整数除法，得到每折的样本数量（最后一折可能多 1-2 个样本，因整除可能有余数，但这里简化为平均分配）。
    # 例如：总样本 1460，k=5，则 fold_size=292（1460÷5=292），即每折 292 个样本。
    
    X_train,y_train = None,None
    #创建两个临时变量，用于存储 “除当前验证集外的所有样本”（即训练集），初始化为 None 是为了后续判断是否为第一次拼接。
   
    for j in range(k):
        #遍历 K 折（j 从 0 到 k-1），每一轮处理第 j 折的数据
        
        idx = slice(j*fold_size,(j+1)*fold_size)
         #slice(start, end) 生成一个切片对象，表示从 start 到 end-1 的索引范围（左闭右开）。
        # 计算第 j 折的样本索引：从 j * fold_size 到 (j+1) * fold_size。
        # 例如：j=0 时，idx=slice(0, 292)（第 0-291 个样本）；j=1 时，idx=slice(292, 584)（第 292-583 个样本），以此类推。
       
        X_part,y_part = X[idx,:],y[idx]
        #根据切片 idx 从总数据中提取第 j 折的特征 X_part 和标签 y_part：
        # X[idx, :]：取 X 中 idx 对应的行（样本），所有列（特征）。
        # y[idx]：取 y 中 idx 对应的行（标签）。
        
        if j == i:
            X_valid,y_valid = X_part,y_part
            #当 j 等于当前折数 i 时，第 j 折作为验证集，
            #将 X_part 和 y_part 赋值给 X_valid 和 y_valid
            
        elif X_train is None:
            X_train,y_train = X_part,y_part
            #若 j 不是当前验证集折，且是第一次遇到非验证集折（X_train 仍为 None），
            #则直接将 X_part 和 y_part 作为训练集的初始部分。
            
        else:
            X_train = torch.cat([X_train,X_part],0)
            y_train = torch.cat([y_train,y_part],0)
            #若 j 不是当前验证集折，且已有初始训练集（X_train 不为 None），则用 torch.cat 将当前折的样本拼接到训练集中：
            # torch.cat([a, b], 0) 表示在第 0 维（行维度）拼接两个张量（即纵向堆叠样本）。
            # 例如：第一次拼接后 X_train 有 292 个样本，第二次拼接后有 584 个样本，直到拼接完所有非验证集折（共 k-1 折）
    return X_train,y_train,X_valid,y_valid
    #循环结束后，返回当前折 i 对应的训练集（k-1 折合并）和验证集（第 i 折）。
```


```python
def k_fold(k,X_train,y_train,num_epochs,learning_rate,weight_decay,batch_size):
   
    train_l_sum,valid_l_sum = 0,0
    #创建两个变量，用于累加 K 折的训练集和验证集性能指标（最后求平均）：
    # train_l_sum：累加每折训练集的最终 Log RMSE。valid_l_sum：累加每折验证集的最终 Log RMSE。
    
    for i in range(k):
        
        data = get_k_fold_data(k,i,X_train,y_train)
        # 调用 get_k_fold_data 函数，获取第 i 折对应的 “训练集特征、训练集标签、验证集特征、验证集标签”，打包为元组 data。
        # data 的结构：(X_train_i, y_train_i, X_valid_i, y_valid_i)，
        #其中 X_train_i 是第 i 折的训练特征，X_valid_i 是第 i 折的验证特征。
        
        net = get_net()
        #调用 get_net 函数创建一个新的模型实例（每折都用全新的模型，避免前一折的参数影响当前折）。
        
        train_ls,valid_ls = train(net,*data,num_epochs,learning_rate,
        weight_decay,batch_size)
        #调用 train 函数训练模型，参数说明：
        # *data：解包元组 data，将 (X_train_i, y_train_i, X_valid_i, y_valid_i) 作为位置参数传入 train 函数。
        # 其余参数是训练超参数（轮数、学习率等）。
        # 返回值：train_ls 是当前折训练集每轮的 Log RMSE，valid_ls 是当前折验证集每轮的 Log RMSE。
        
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1]
        #train_ls[-1]：当前折训练结束后，最后一轮（第 num_epochs 轮）的训练集 Log RMSE（代表该折模型的最终训练性能）。
        # valid_ls[-1]：当前折训练结束后，最后一轮的验证集 Log RMSE（代表该折模型的最终泛化性能）。
        # 将这两个值分别累加到 train_l_sum 和 valid_l_sum 中，用于后续计算平均值。
        
        if i == 0:
            d2l.plot(list(range(1,num_epochs + 1)),[train_ls,valid_ls],
                     xlabel = 'epoch',ylabel='rmse',xlim=[1,num_epochs],
                     legend=['train','valid'],yscale='log')
            #仅在第一折（i=0）训练后绘制曲线，避免重复绘图：
            # x 轴：训练轮数（1 到 num_epochs）。
            # y 轴：Log RMSE（用对数刻度 yscale='log' 更清晰展示误差变化）。
            # 曲线：蓝色为训练集误差，绿色为验证集误差，用于直观观察模型是否过拟合（如训练误差下降但验证误差上升）。
        print(f'折{i+1},训练log rmse{float(train_ls[-1]):f},'
                  f'验证log rmse{float(valid_ls[-1]):f}')
    return train_l_sum/k,valid_l_sum/k



```


```python
k,num_epochs,lr,weight_decay,batch_size = 5,100,5,0,64
train_l,valid_l = k_fold(k,train_features,train_labels,num_epochs,lr,weight_decay,batch_size)
print(f'{k}-折验证:平均训练log rmse:{float(train_l):f},'
      f'平均验证log rmse:{float(valid_l):f}')
```

    折1,训练log rmse0.169836,验证log rmse0.156527
    折2,训练log rmse0.161912,验证log rmse0.188202
    折3,训练log rmse0.163523,验证log rmse0.167799
    折4,训练log rmse0.168331,验证log rmse0.154366
    折5,训练log rmse0.162845,验证log rmse0.182543
    5-折验证:平均训练log rmse:0.165289,平均验证log rmse:0.169887
    


    
![svg](output_17_1.svg)
    



```python
def train_and_pred(train_features,test_features,train_labels,test_data,num_epochs,lr,weight_decay,batch_size):
    net = get_net()
    train_ls,_ = train(net,train_features,train_labels,None,None,num_epochs,lr,weight_decay,batch_size)
    d2l.plot(np.arange(1,num_epochs+1),[train_ls],xlabel='epoch',ylabel='log rmse',xlim = [1,num_epochs],yscale='log')
    print(f'训练log rmse:{float(train_ls[-1]):f}')
    #将网络应用到训练集
    preds = net(test_features).detach().numpy()
    #将其重新格式化以导出到kaggle
    test_data['SalePrice'] = pd.Series(preds.reshape(1,-1)[0])
    submission = pd.concat([test_data['Id'],test_data['SalePrice']],axis=1)
    submission.to_csv('submission.csv',index=False)
```


```python
train_and_pred(train_features,test_features,train_labels,test_data,num_epochs,lr,weight_decay,batch_size)
```

    训练log rmse:0.162406
    


    
![svg](output_19_1.svg)
    



```python

```
