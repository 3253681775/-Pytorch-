```python
import torch
from torch import nn
from d2l import torch as d2l 
net = nn.Sequential(
    #这里使用一个11*11的更大窗口来捕获对象
    #同时，步幅是4，以减少输出的高度和宽度
    #另外，输出通道远小于LeNet
    nn.Conv2d(1,96,kernel_size = 11,stride=4,padding=1),
    nn.ReLU(),
    #in_channels=1：输入通道数为 1，因为 Fashion-MNIST 是灰度图（单通道），原 AlexNet 为 3（RGB 彩色图）。
    # out_channels=96：输出通道数为 96，即卷积核的数量（原 AlexNet 是 96×2，因双 GPU 训练），每个卷积核提取一种特征。
    # kernel_size=11：卷积核大小为 11×11，大卷积核是 AlexNet 的特点之一，用于捕获图像的全局特征（相比 LeNet 的 5×5）。
    # stride=4：步幅为 4，卷积时每次移动 4 个像素，大幅减小输出的高和宽，降低计算量。
    #输出尺寸=向下取整((输入尺寸+2*padding-kernel_size)/stride)+1
    #ReLU 激活函数，替换了传统的 Sigmoid/Tanh，解决了梯度消失问题，加速训练。

    
    nn.MaxPool2d(kernel_size=3,stride=2),
    # kernel_size=3：池化核大小为 3×3。stride=2：步幅为 2，进一步减小特征图的尺寸，
    # 保留关键特征的同时降低计算量。输入为 5×5×96，
    
    # #减小卷积窗口，使用填充为2来使得输入和输出的高和宽一至，且增发输出通道
    nn.Conv2d(96,256,kernel_size=5,padding=2),nn.ReLU(),
    nn.MaxPool2d(kernel_size=3,stride=2),
    #使用3格连续的卷积层和较小的卷积窗口
    #除了最后的卷积层，输出通道数进一步减小
    #在前两个卷积层之后，汇聚层不用于减小输入的宽度和高度
    nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(),
    nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(),
    nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLU(),
    nn.MaxPool2d(kernel_size=3,stride=2),
    nn.Flatten(),
    #这里，全连接层的输出数量是LeNet中的好几倍，使用暂退层来缓解过拟合
    nn.Linear(6400,4096),nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096,4096),nn.ReLU(),
    nn.Dropout(p=0.5),
    #最后是处处层，因为使用Fashion -MNIST,所以类别是是10，为非论文中的1000
    nn.Linear(4096,10))
```


```python
X = torch.randn(1,1,224,224)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)
```

    Conv2d output shape:	 torch.Size([1, 96, 54, 54])
    ReLU output shape:	 torch.Size([1, 96, 54, 54])
    MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])
    Conv2d output shape:	 torch.Size([1, 256, 26, 26])
    ReLU output shape:	 torch.Size([1, 256, 26, 26])
    MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])
    Conv2d output shape:	 torch.Size([1, 384, 12, 12])
    ReLU output shape:	 torch.Size([1, 384, 12, 12])
    Conv2d output shape:	 torch.Size([1, 384, 12, 12])
    ReLU output shape:	 torch.Size([1, 384, 12, 12])
    Conv2d output shape:	 torch.Size([1, 256, 12, 12])
    ReLU output shape:	 torch.Size([1, 256, 12, 12])
    MaxPool2d output shape:	 torch.Size([1, 256, 5, 5])
    Flatten output shape:	 torch.Size([1, 6400])
    Linear output shape:	 torch.Size([1, 4096])
    ReLU output shape:	 torch.Size([1, 4096])
    Dropout output shape:	 torch.Size([1, 4096])
    Linear output shape:	 torch.Size([1, 4096])
    ReLU output shape:	 torch.Size([1, 4096])
    Dropout output shape:	 torch.Size([1, 4096])
    Linear output shape:	 torch.Size([1, 10])
    


```python
batch_size = 128
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=224)
#实现图像缩放，将 28×28 的小图拉伸为 224×224 的大图，确保输入与 AlexNet 的网络结构兼容。
# train_iter：训练集迭代器
# 包含 Fashion-MNIST 的60000 个训练样本（图像 + 标签）；
# 训练过程中，通过for X, y in train_iter:循环可逐批次读取训练数据：
# X：形状为(batch_size, 1, 224, 224)的张量，表示批次中的图像数据（1 为灰度图通道数，224×224 为调整后的尺寸）；
# y：形状为(batch_size,)的张量，表示批次中图像对应的标签（0-9，对应 10 类服装）。
# 2. test_iter：测试集迭代器
# 包含 Fashion-MNIST 的10000 个测试样本（图像 + 标签）；
# 测试 / 验证过程中，通过同样的循环读取测试数据，用于评估模型的泛化能力
```


```python
lr,num_epochs = 0.01,10
d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())

# 图像张量：形状为(batch_size, 通道数, 高, 宽)，存储的是像素的数值信息，调整大小是对这个张量的 ** 空间维度（高 / 宽）** 进行插值 / 缩放操作（如将 28×28 拉伸为 224×224）。
# 标签张量：形状为(batch_size,)，存储的是 0-9 的整数类别值，这个张量没有空间维度，也不会参与任何图像缩放的操作。
# # 举个例子：一张标签为7（运动鞋）的 28×28 图像，拉伸为 224×224 后，图像的像素排列变了，但它的内容依然是 “运动鞋”，因此标签仍然是7，不会变为其他数值。

# 在你使用的train_iter = d2l.load_data_fashion_mnist(...)中，
#train_iter并不是包含两个元组，而是一个PyTorch 的DataLoader对象（数据迭代器）。
#当你通过for X, y in train_iter遍历它时，每次迭代会得到一个包含两个张量的元组（而非train_iter本身是两个元组）
```


```python

```
