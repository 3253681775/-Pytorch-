```python
import torch
from torch import nn
from torch.nn import functional as F
net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))
X = torch.rand(2,20)
net(X)
#多层感知机

```




    tensor([[ 0.0678,  0.3581,  0.0131,  0.1440,  0.3568,  0.2492,  0.0567,  0.0231,
             -0.1850,  0.2754],
            [-0.0259,  0.3454,  0.0305,  0.2073,  0.4021,  0.2353, -0.0287,  0.0715,
             -0.2439,  0.1722]], grad_fn=<AddmmBackward0>)




```python
#自定义块
class MLP(nn.Module):
    #用模型参数声明层这里，我们声明两个全连接的层
    def __init__(self):
        #调用MLP的父类Moudle的构造函数来执行必要的初始化
        #这样，在类实例化时也可以指定该其他函数参数，例如模型参数params
        super().__init__()
        self.hidden = nn.Linear(20,256)#隐藏层
        self.out = nn.Linear(256,10)#输出层

    #定义模型的前向传播，既如何根据输入X返回所需的模型输出
    def forward(self,X):
        #注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义
        return self.out(F.relu(self.hidden(X)))
        
```


```python
net = MLP()
net(X)
```




    tensor([[ 0.0061, -0.1197, -0.0152, -0.1156, -0.1392,  0.1822, -0.0368,  0.1818,
             -0.0609, -0.0482],
            [-0.1394, -0.1384, -0.0441, -0.0398, -0.1794,  0.1474, -0.0697,  0.1578,
             -0.0094,  0.0542]], grad_fn=<AddmmBackward0>)




```python
#顺序块
class MySequential(nn.Module):
    #继承 nn.Module 后，该类会自动获得神经网络模块的核心功能（如参数管理、设备迁移等）
    
    def __init__(self,*args):
        #*args 是可变参数，用于接收任意数量的神经网络层（nn.Module 的子类实例，如 nn.Linear、nn.Conv2d 等）
        super().__init__()
        #调用父类 nn.Module 的初始化方法，确保父类的成员变量和功能被正确初始化（例如 _modules 字典的创建）。
        for idx,module in enumerate(args):
            #这里，module是Module子类的一个实例我们把它保存在Module类的成员
            #self._modules中。_module的类型是OrderedDict
            
            self._modules[str(idx)] = module
            #self._modules 是 nn.Module 内置的一个特殊字典（类型为 OrderedDict，有序字典），
            #用于存储当前模块的子模块。
            
    def forward(self,X):
        #OrderedDict保证了按照成员添加的顺序遍历他们
        for block in self._modules.values():
            #self._modules.values() 按插入顺序（即初始化时传入的顺序）获取所有子模块
            #（因 _modules 是 OrderedDict，保证顺序）。
            X = block(X)
            #遍历每个子模块 block，将当前输入 X 传入子模块进行计算（block(X)），并将输出重新赋值给 X。
        return X
```


```python
net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))
net(X)
```




    tensor([[-0.0752, -0.2062,  0.0987,  0.0522,  0.1617, -0.2373,  0.0725,  0.0963,
              0.1249,  0.1481],
            [-0.0505, -0.2361, -0.0128,  0.1501,  0.0267, -0.2739,  0.0164,  0.0353,
              0.1366,  0.1016]], grad_fn=<AddmmBackward0>)




```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        #不计算梯度的随机权重参数，因此其在训练期间保持不变
        self.rand_weight = torch.rand((20,20),requires_grad=False)
        self.linear = nn.Linear(20,20)

    def forward(self,X):
        X = self.linear(X)
        #使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X,self.rand_weight)+1)
        X = self.linear(X)
        #控制流
        while X.abs().sum() > 1:
            X /=2
        return X.sum()
```


```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),
                                nn.Linear(64,32),nn.ReLU())
        self.linear = nn.Linear(32,16)

    def forward(self,X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())
chimera(X)
```




    tensor(0.0639, grad_fn=<SumBackward0>)




```python

```


```python

```
