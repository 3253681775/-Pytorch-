```python
import  math
import torch
from torch import nn
from d2l import torch as d2l
```


```python
num_hiddens,num_heads = 100,5
attention = d2l.MultiHeadAttention(num_hiddens,num_hiddens,num_hiddens,
                                   num_hiddens,num_hiddens,0.5)
attention.eval()
```




    MultiHeadAttention(
      (attention): DotProductAttention(
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (W_q): Linear(in_features=100, out_features=100, bias=False)
      (W_k): Linear(in_features=100, out_features=100, bias=False)
      (W_v): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=100, out_features=100, bias=False)
    )




```python
batch_size,num_queries,valid_lens = 2,4,torch.tensor([3,2])
X = torch.ones((batch_size,num_queries,num_hiddens))
attention(X,X,X,valid_lens).shape
```




    torch.Size([2, 4, 100])




```python
#位置编码
class PositionalEncoding(nn.Module):
    """位置编码"""
    def __init__(self,num_hiddens,dropout,max_len=1000):
        super(PositionalEncoding,self).__init__()
        self.dropout = nn.Dropout(dropout)
        #创建一个足够长的p
        self.P = torch.zeros((1,max_len,num_hiddens))
        #         形状：(1, max_len, num_hiddens)；
        # 1：批次维度（适配任意 batch_size，广播机制）；
        # max_len：预先生成的最大序列长度（比如 1000）；
        # num_hiddens：位置编码的维度（与输入特征维度一致）；
        
        X = torch.arange(max_len,dtype=torch.float32).reshape(-1,1)/torch.pow(10000,torch.arange(
        0,num_hiddens,2,dtype=torch.float32)/num_hiddens)
        # torch.arange(max_len, dtype=torch.float32)：生成位置序列 [0,1,2,...,max_len-1]，形状 (max_len,)；
        # .reshape(-1,1)：将位置序列转为列向量，形状 (max_len, 1)（适配后续广播计算）
        # torch.arange(0, num_hiddens, 2, dtype=torch.float32)：生成偶数维度索引 [0,2,4,...,num_hiddens-2]（比如 num_hiddens=8 时为 [0,2,4,6]），形状 (num_hiddens//2,)；
        # / num_hiddens：将维度索引归一化到 [0,1) 区间，对应公式中的 2i/d_model；
        # torch.pow(10000, ...)：计算 \(10000^{2i/d_{\text{model}}}\)，形状 (num_hiddens//2,)；
#         # 最终除法：位置序列 (max_len,1) ÷ 幂次 (num_hiddens//2,) → 广播为 (max_len, num_hiddens//2)，记为 X。
#         示例：若 max_len=3、num_hiddens=4：
        # 位置序列：[0,1,2] → reshape 后 [[0],[1],[2]]；
        # 偶数维度索引：[0,2] → 归一化后 [0/4, 2/4] = [0, 0.5]；
        # 幂次：10000^0=1、10000^0.5=100；
        # X = [[0/1, 0/100], [1/1, 1/100], [2/1, 2/100]] = [[0,0], [1,0.01], [2,0.02]]。
        # 示例中 X 的形状：(3, 2)（3 个位置 × 2 个偶数维度系数）
        
        self.P[:,:,0::2] = torch.sin(X)
        self.P[:,:,1::2] = torch.cos(X)
#         初始化后 self.P 的初始状态：shape=(1, 3, 4)，所有值为 0，即：
# plaintext
# self.P = [
#     [
#         [0.0, 0.0, 0.0, 0.0],  # 位置0的编码（4维）
#         [0.0, 0.0, 0.0, 0.0],  # 位置1的编码（4维）
#         [0.0, 0.0, 0.0, 0.0]   # 位置2的编码（4维）
#     ]
# ]

    def forward(self,X):
        X = X + self.P[:,:X.shape[1],:].to(X.device)
        return self.dropout(X)
```


```python
# 步骤 1：理解 self.P[:,:,0::2] 的切片含义
# self.P 的形状是 (1, max_len, num_hiddens) = (1, 3, 4)，切片 [:,:,0::2] 按维度顺序（批次、位置、编码维度） 拆解：
# 切片部分	含义	选中的维度 / 位置
# :	批次维度：选中所有批次（此处只有 1 个批次）	批次 0
# :	位置维度：选中所有位置（此处是位置 0、1、2）	位置 0、1、2
# 0::2	编码维度：从索引 0 开始，步长为 2（即偶数维度）	编码维度 0、2（对应 2i，i=0、1）
# 因此，self.P[:,:,0::2] 选中的是：
# 批次 0、所有 3 个位置、编码维度 0 和 2；
# 选中部分的形状：(1, 3, 2)（1 个批次 × 3 个位置 × 2 个偶数维度）。
# 步骤 2：计算 torch.sin(X) 的数值（核心）
# X 是形状为 (3, 2) 的张量，torch.sin(X) 对每个元素取正弦值，计算结果如下（保留 4 位小数）：
# plaintext
# torch.sin(X) = [
#     [sin(0.0),  sin(0.0)],   # 位置0：sin(0)=0.0, sin(0)=0.0
#     [sin(1.0),  sin(0.01)],  # 位置1：sin(1)≈0.8415, sin(0.01)≈0.0100
#     [sin(2.0),  sin(0.02)]   # 位置2：sin(2)≈0.9093, sin(0.02)≈0.0200
# ]
# 计算后 torch.sin(X) 的形状仍为 (3, 2)。
# 步骤 3：广播机制匹配形状（关键）
# self.P[:,:,0::2] 的形状是 (1, 3, 2)，而 torch.sin(X) 的形状是 (3, 2)——PyTorch 会自动触发广播机制，将 torch.sin(X) 扩展为 (1, 3, 2)，扩展后的值与原 torch.sin(X) 完全一致（仅增加了批次维度）：
# plaintext
# 广播后的 torch.sin(X) = [
#     [
#         [0.0,    0.0],   # 位置0：维度0、2的正弦值
#         [0.8415, 0.01],  # 位置1：维度0、2的正弦值
#         [0.9093, 0.02]   # 位置2：维度0、2的正弦值
#     ]
# ]
# 步骤 4：赋值过程（最终填充）
# 将广播后的 torch.sin(X) 赋值给 self.P[:,:,0::2]，即把正弦值填充到 self.P 的偶数维度（0、2），赋值后 self.P 的状态变为：
# plaintext
# self.P = [
#     [
#         [0.0,    0.0,    0.0,    0.0],  # 位置0：维度0=0.0（正弦）、维度1=0、维度2=0.0（正弦）、维度3=0
#         [0.8415, 0.0,    0.01,   0.0],  # 位置1：维度0=0.8415（正弦）、维度1=0、维度2=0.01（正弦）、维度3=0
#         [0.9093, 0.0,    0.02,   0.0]   # 位置2：维度0=0.9093（正弦）、维度1=0、维度2=0.02（正弦）、维度3=0
#     ]
# ]
```


```python
encoding_dim,num_steps = 32,60
pos_encoding = PositionalEncoding(encoding_dim,0)
pos_encoding.eval()
X = pos_encoding(torch.zeros((1,num_steps,encoding_dim)))
P = pos_encoding.P[:,:X.shape[1],:]
d2l.plot(torch.arange(num_steps),P[0,:,6:10].T,xlabel='Row(position)',
         figsize=(6,2.5),legend=["Col %d" % d for d in torch.arange(6,10)])
# x轴：位置0-59
 # y轴：6-9列（转置后4行，对应4条曲线）
# "Col %d" % d：Python 旧式字符串格式化（%d 匹配整数），将每个列索引 d 拼接成 "Col 6"、"Col 7" 等字符串；
# d2l.plot(x, y, xlabel=None, ylabel=None, legend=None, figsize=None, title=None)
# 二、核心参数说明
# 参数	类型	说明
# x	张量 / 列表 / 数组	x 轴数据（一维），若为多个曲线，可传入二维数据（每行对应一条曲线的 x）
# y	张量 / 列表 / 数组	y 轴数据（一维 / 二维），二维时每行对应一条曲线，与 x 维度匹配
# xlabel	字符串	x 轴标签（如 "Step"、"Position"）
# ylabel	字符串	y 轴标签（如 "Value"、"Loss"）
# legend	列表（字符串）	图例标签，长度需与 y 的行数一致（如 ["Curve1", "Curve2"]）
# figsize	元组 (width, height)	绘图窗口尺寸（单位：英寸），如 (6, 2.5)
# title	字符串	图表标题
```


    
![svg](output_5_0.svg)
    



```python
#绝对位置信息
for i in range(8):
    print(f'{i}的二进制是:{i:>03b}')
    # 格式化扩展：
# 若要打印n位二进制：{i:>0nb}（如08b表示 8 位补零）；
# 左对齐：{i:<03b}（输出000/100/010...）；
# 无补零：{i:b}（输出0/1/10...）。

P = P[0,:,:].unsqueeze(0).unsqueeze(0)
d2l.show_heatmaps(P,xlabel='Column(encoding dimension)',
                  ylabel='Row(position)',figsize=(3.5,4),cmap='Blues')
```

    0的二进制是:000
    1的二进制是:001
    2的二进制是:010
    3的二进制是:011
    4的二进制是:100
    5的二进制是:101
    6的二进制是:110
    7的二进制是:111
    


    
![svg](output_6_1.svg)
    



```python

```
