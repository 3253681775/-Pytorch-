```python
#加载和保存张量
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
#torch.arange(4) 生成一个包含 0 到 3 的 1 维张量（形状为 torch.Size([4])）。
# 赋值给变量 x，此时 x 的值为：tensor([0, 1, 2, 3])。

torch.save(x,'x-file')
#torch.save(obj, f) 是 PyTorch 用于保存对象的函数：
# 第一个参数 x：要保存的对象（这里是张量 x）。
# 第二个参数 'x-file'：保存路径和文件名（当前目录下创建名为 x-file 的文件）。
# 执行后，张量 x 的数据会被序列化并写入 x-file 文件中，实现持久化存储（程序退出后仍存在）。

x2 = torch.load('x-file')
#torch.load(f) 是 PyTorch 用于加载保存对象的函数：
# 参数 'x-file'：要加载的文件路径（即之前保存的文件）。
# 函数会读取文件内容，反序列化并恢复为原来的张量对象。
# 赋值给变量 x2，此时 x2 是从文件中恢复的张量，与原张量 x 的值完全一致。

x2
```




    tensor([0, 1, 2, 3])




```python
y = torch.zeros(4)

torch.save([x,y],'x-files')

x2,y2 = torch.load('x-files')
#当使用 torch.save(obj, 'x-files') 时，如果当前目录下已经存在名为 'x-files' 的文件，PyTorch 会直接覆盖该文件，不会有提示。

(x2,y2)
```




    (tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))




```python
mydict = {'x':x,'y':y}
torch.save(mydict,'mydict')
mydict2 = torch.load('mydict')
mydict2
```




    {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}




```python
#加载和保存模型参数
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20,256)
        self.output = nn.Linear(256,10)

    def forward(self,x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size = (2,20))
Y = net(X)
```


```python
torch.save(net.state_dict(),'mlp.params')
#net.state_dict()：获取模型的参数状态字典（包含所有可训练参数，如hidden.weight、output.bias等）。
```


```python
clone = MLP()
#实例化一个新的MLP模型clone，其结构与net完全一致，但初始参数是随机的（与net不同）。

clone.load_state_dict(torch.load('mlp.params'))
#torch.load('mlp.params')：从文件中加载之前保存的参数字典。
#clone.load_state_dict(...)：将加载的参数字典赋值给clone模型，覆盖其随机初始化的参数。

clone.eval()
#eval()：是nn.Module的方法，用于将模型切换到评估（推理）模式
#作用：关闭 dropout、批量归一化的动态更新等训练特有的行为，确保推理时结果稳定
```




    MLP(
      (hidden): Linear(in_features=20, out_features=256, bias=True)
      (output): Linear(in_features=256, out_features=10, bias=True)
    )




```python

```
