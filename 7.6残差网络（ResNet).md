```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

```


```python
class Residual(nn.Module):
    def __init__(self,input_channels,num_channels,use_1x1conv=False,strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels,num_channels,
                               kernel_size=3,padding=1,stride=strides)
        self.conv2 = nn.Conv2d(num_channels,num_channels,
                               kernel_size=3,padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels,num_channels,
                               kernel_size=1,stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self,X):
        Y = F.relu(self.bn1(self.conv1(X)))
        #函数F.relu：PyTorch 的函数式 ReLU 激活（也可使用nn.ReLU()模块，效果一致）
        Y = self.bn2(self.conv2(Y))
        #关键设计：此处不使用 ReLU 激活，而是在跳跃连接相加后再激活（return F.relu(Y)），
        #这是 ResNet 残差块的标准配置，避免激活函数破坏残差的梯度传播。
        if self.conv3:
            X = self.conv3(X)
            #核心逻辑：若conv3存在（use_1x1conv=True），
            #对输入X做 1×1 卷积，调整其通道数和空间尺寸，使其与残差映射的输出Y维度一致；
        Y += X
        #1. 批量归一化（BN 层）的强约束代码中conv1和conv2后都紧跟了nn.BatchNorm2d层，
        #这是控制数值范围的最核心手段：BN 层的作用是将每个通道的特征归一化为均值接近 0、方差接近 1的分布，
        #公式为：\(\hat{X} = \frac{X - \mu_{batch}}{\sqrt{\sigma^2_{batch} + \epsilon}} \times \gamma + \beta\)
        #即使经过\(\gamma\)（缩放）和\(\beta\)（偏移）的调整，特征值也会被约束在有限的小范围内（通常在\([-5, 5]\)之间）。
        #残差映射\(\mathcal{F}(X)\)是两个卷积 + BN 层的输出，
        #其数值已经被 BN 层严格约束，因此\(\mathcal{F}(X)\)的取值不会过大，
        #与X相加后自然也不会出现数值爆炸。
        return F.relu(Y)
```


```python
# blk = Residual(3,3)
# X = torch.rand(4,3,6,6)
# Y = blk(X)
# #执行Y = blk(X)时会自动触发残差块的前向传播，
# # 核心原因是 **Residual类继承了nn.Module基类，
# # 而nn.Module重载了 Python 的__call__魔法方法 **，
# #最终调用我们定义的forward方法完成前向传播

# #1. forward方法为什么不能直接调用？
# # 虽然可以直接执行Y = blk.forward(X)得到结果，但强烈不建议这样做，原因是：
# # __call__方法中除了调用forward，还会执行钩子函数、处理nn.Parameter的设备迁移、记录计算图（用于反向传播）等关键操作；
# # 直接调用forward会跳过这些步骤，导致模型的钩子失效、反向传播异常（如无法计算梯度）。
# Y.shape
```




    torch.Size([4, 3, 6, 6])




```python
# #也可以增加输出通道数的同时，减半输出的高度和宽度
# blk = Residual(3,6,use_1x1conv=True,strides=2)
# blk(X).shape
```




    torch.Size([4, 6, 3, 3])




```python
#ResNet模型
b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),
                   nn.BatchNorm2d(64),nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
#b1对输入的处理是单次的、无重复的，仅在网络最前端执行一次，目的是快速将原始图像（224×224 单通道）转化为适合残差层处理的特征图（56×56，64 通道）。
```


```python
def resnet_block(input_channels,num_channels,num_residuals,first_block=False):
    blk=[]
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels,num_channels,
                                use_1x1conv=True,strides=2))
        else:
            blk.append(Residual(num_channels,num_channels))
    return blk

b2 = nn.Sequential(*resnet_block(64,64,2,first_block=True))
b3 = nn.Sequential(*resnet_block(64,128,2))
b4 = nn.Sequential(*resnet_block(128,256,2))
b5 = nn.Sequential(*resnet_block(256,512,2))

net = nn.Sequential(b1,b2,b3,b4,b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(),nn.Linear(512,10))
```


```python
X = torch.rand(size = (1,1,224,224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)
```

    Sequential output shape:	 torch.Size([1, 64, 56, 56])
    Sequential output shape:	 torch.Size([1, 64, 56, 56])
    Sequential output shape:	 torch.Size([1, 128, 28, 28])
    Sequential output shape:	 torch.Size([1, 256, 14, 14])
    Sequential output shape:	 torch.Size([1, 512, 7, 7])
    AdaptiveAvgPool2d output shape:	 torch.Size([1, 512, 1, 1])
    Flatten output shape:	 torch.Size([1, 512])
    Linear output shape:	 torch.Size([1, 10])
    


```python
lr,num_epochs,batch_size = 0.05,10,256
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=96)
d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())
```
