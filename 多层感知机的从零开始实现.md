```python
import torch
from torch import nn
from d2l import torch as d2l
```


```python
batch_size = 256
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)

```


```python
num_inputs,num_outputs,num_hiddens =784,10,256
W1 = nn.Parameter(torch.randn(
    num_inputs,num_hiddens,requires_grad=True)*0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens,num_outputs,requires_grad=True)*0.01)
b2 = nn.Parameter(torch.zeros(num_outputs,requires_grad=True))

params = [W1,b1,W2,b2]
#num_inputs, num_outputs, num_hiddens =784,10,256
#定义网络各层的维度：
#num_inputs=784：输入特征数，因为 Fashion-MNIST 图像是 28×28=784 像素
#num_outputs=10：输出类别数，对应 10 种时尚物品
#num_hiddens=256：隐藏层神经元数量，这是一个超参数

#W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True)*0.01)
#定义第一层（输入层到隐藏层）的权重参数：
#使用 nn.Parameter 包装，标记为模型可学习的参数
#torch.randn 生成符合标准正态分布的随机数#形状为 (num_inputs, num_hiddens) 即 (784, 256)
#requires_grad=True 表示需要计算梯度，用于反向传播更新
#乘以 0.01 是为了缩小初始权重值，有助于训练稳定#

# b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
# 定义第一层的偏置参数：
# 初始化为全 0 张量，形状为 (num_hiddens,) 即 (256,)
# 同样标记为可学习参数并需要计算梯度

# W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True)*0.01)
# 定义第二层（隐藏层到输出层）的权重参数：
# 形状为 (num_hiddens, num_outputs) 即 (256, 10)
# 初始化方式与 W1 相同
# b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
# 定义第二层的偏置参数：
# 初始化为全 0 张量，形状为 (num_outputs,) 即 (10,)

# params = [W1,b1,W1,b2]
# 创建包含所有模型参数的列表，方便后续传入优化器

```


```python
#nn.Parameter核心作用
#标记可学习参数：当一个张量被 nn.Parameter 包裹后，PyTorch 会自动将其识别为模型的可学习参数，
#在调用 model.parameters() 时会被包含在内，便于后续通过优化器（如 SGD、Adam）进行更新。

#自动注册到模块：如果将 nn.Parameter 实例作为 nn.Module 子类（自定义网络层）的属性，
#它会被自动注册到该模块的参数列表中，无需手动管理。

#默认需要梯度nn.Parameter 创建的张量默认 requires_grad=True（可通过参数修改），
#表示需要计算梯度，用于反向传播时更新参数。
```


```python
#激活函数
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X,a)
```


```python
#模型
def net(X):
    X = X.reshape(-1,num_inputs)
    H = relu(X@W1 + b1) #这里@表示矩阵乘法
    return (H@W2 + b2)
```


```python
#损失函数
loss = nn.CrossEntropyLoss(reduction = 'none')
```


```python
num_epochs,lr = 10,0.1
updater = torch.optim.SGD(params,lr=lr)
d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,updater)
```


    
![svg](output_7_0.svg)
    



```python
d2l.predict_ch3(net,test_iter)
```


    
![svg](output_8_0.svg)
    



```python

```
