```python
import math
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l
```


```python
#基于位置的前馈网络
class PositionWiseFFN(nn.Module):
    """基于位置的前馈网络"""
    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_outputs,**kwargs):
        super(PositionWiseFFN,self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input,ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens,ffn_num_outputs)

    def forward(self,X):
        return self.dense2(self.relu(self.dense1(X)))
        
```


```python
ffn = PositionWiseFFN(4,4,8)
ffn.eval()
ffn(torch.ones((2,3,4)))[0]
# 子部分 3.3：[0]（结果切片）
# 作用：取前向传播输出张量的「第 0 个样本」（batch 维度的第一个元素）。
# 形状变化：
# # 前向传播输出形状是(2,3,8)，切片[0]后形状变为(3,8) → 对应 “第一个样本的 3 个位置，每个位置 8 维输出特征”。
```




    tensor([[ 0.1271,  0.2236,  0.3790, -0.3364, -0.2705,  0.3522, -0.3837,  0.3655],
            [ 0.1271,  0.2236,  0.3790, -0.3364, -0.2705,  0.3522, -0.3837,  0.3655],
            [ 0.1271,  0.2236,  0.3790, -0.3364, -0.2705,  0.3522, -0.3837,  0.3655]],
           grad_fn=<SelectBackward0>)




```python
# 残差连接和层规范化
ln = nn.LayerNorm(2)
#计算 LayerNorm (2) 的输出（对每个样本独立规范化）这里的样本指的是一行
#输入的参数是特征维度的大小，同下，一般都是最后一个维度的大小

bn = nn.BatchNorm1d(2)
# 计算 BatchNorm1d (2) 的输出（对每个特征跨样本规范化）

X = torch.tensor([[1,2],[2,3]],dtype=torch.float32)
#在训练模式下计算X的均值和方差
print('layer norm:',ln(X),'\nbatch norm:',bn(X))
```

    layer norm: tensor([[-1.0000,  1.0000],
            [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) 
    batch norm: tensor([[-1.0000, -1.0000],
            [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)
    


```python
#残差连接后进行层规范化
class AddNorm(nn.Module):
    def __init__(self,normalized_shape,dropout,**kwargs):
        super(AddNorm,self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self,X,Y):
        return self.ln(self.dropout(Y) + X)
```


```python
add_norm = AddNorm([3,4],0.5)
add_norm.eval()
add_norm(torch.ones((2,3,4)),torch.ones((2,3,4))).shape
```




    torch.Size([2, 3, 4])




```python
#编码器
class EncoderBlock(nn.Module):
    """Transformer编码器块"""
    def __init__(self,key_size,query_size,value_size,num_hiddens,
                 norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,
                 dropout,use_bias=False,**kwargs):
        super(EncoderBlock,self).__init__(**kwargs)
        
        self.attention = d2l.MultiHeadAttention(
        key_size,query_size,value_size,num_hiddens,num_heads,dropout,
        use_bias)
        
        self.addnorm1 = AddNorm(norm_shape,dropout)
        
        self.ffn = PositionWiseFFN(
        ffn_num_input,ffn_num_hiddens,num_hiddens)
        
        self.addnorm2 = AddNorm(norm_shape,dropout)

    def forward(self,X,valid_lens):
        Y = self.addnorm1(X,self.attention(X,X,X,valid_lens))
        return self.addnorm2(Y,self.ffn(Y))
```


```python
X = torch.ones((2,100,24))
valid_lens = torch.tensor([3,2])
encoder_blk = EncoderBlock(24,24,24,24,[100,24],24,48,8,0.5)
encoder_blk.eval()
encoder_blk(X,valid_lens).shape
```




    torch.Size([2, 100, 24])




```python
class TransformerEncoder(d2l.Encoder):
    """Transformer编码器"""
    def __init__(self,vocab_size,key_size,query_size,value_size,
                 num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,
                 num_heads,num_layers,dropout,use_bias=False,**kwargs):
#         vocab_size	int	词汇表大小（如 200，表示词索引范围 0~199）
# key_size	int	注意力机制中 Key 向量的维度（如 24）
# query_size	int	注意力机制中 Query 向量的维度（如 24）
# value_size	int	注意力机制中 Value 向量的维度（如 24）
# ★num_hiddens	int	编码器的核心特征维度（如 24，所有层输出维度统一）
# norm_shape	list/tuple	层规范化的维度（如 [100,24]，对应 seq_len+feature_dim）
# ffn_num_input	int	位置前馈网络的输入维度（如 24，等于 num_hiddens）
# ffn_num_hiddens	int	位置前馈网络的隐藏层维度（如 48）
# num_heads	int	多头注意力的头数（如 8）
# ★num_layers	int	堆叠的编码器块数量（如 2，Transformer 的深度）
# dropout	float	Dropout 丢弃概率（如 0.5）
# use_bias	bool	注意力层是否使用偏置（False，Transformer 默认）
# **kwargs	关键字参数	传给父类d2l.Encoder的参数（如 device/dtype）
        
        super(TransformerEncoder,self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size,num_hiddens)
        # 作用：初始化词嵌入层（Embedding Layer），将整数型的词索引转换为稠密的特征向量。
        # 维度变换：输入(batch_size, seq_len)（如(2,100)，整数序列）→ 输出(batch_size, seq_len, num_hiddens)（如(2,100,24)，24 维词向量）。
        
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens,dropout)
        # 作用：初始化位置编码层（d2l 库实现的 PositionalEncoding），为词向量添加位置信息（Transformer 无 RNN/CNN，需显式编码位置）。
        # 核心逻辑：
        # 位置编码的维度 = 词嵌入维度（24），保证可与词嵌入相加；
        # 位置编码值范围在[-1,1]，通过正弦 / 余弦函数生成，不同位置有唯一编码；
        # 包含 Dropout 层，防止位置编码过拟合。
        
        self.blks = nn.Sequential()
        # 作用：初始化nn.Sequential容器，用于按顺序存储 / 执行多个EncoderBlock（编码器块）。
        # 优势：nn.Sequential会自动按添加顺序执行子层，无需手动管理循环（也可手动循环，此处是规范写法）。
        
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                                 EncoderBlock(key_size,query_size,value_size,
                                              num_hiddens,norm_shape,ffn_num_input,
                                              ffn_num_hiddens,num_heads,dropout,use_bias))
        # 作用：向nn.Sequential中添加第i个编码器块，并命名为block0/block1（方便后续访问）。
        # 参数传递：将初始化函数接收的超参数（如key_size=24、num_heads=8）传递给EncoderBlock，保证所有编码器块参数一致。
        # 关键：所有编码器块共享相同的超参数，仅参数值（权重）独立（训练时各自更新）。
        
    def forward(self,X,valid_lens,*args):
        # X：输入的词索引序列，形状(batch_size, seq_len)（如(2,100)，整数型）；
        # valid_lens：注意力有效长度，形状(batch_size,)（如[3,2]，屏蔽填充位置的注意力）；
        # *args：兼容父类接口的可变参数（无实际作用，预留扩展）。
        
        #因为位置编码范围在-1~1,所以嵌入值乘以嵌入维度的平方根进行缩放，再与位置编码相加
        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))
        # self.embedding(X)：词嵌入 → 形状(2,100,24)；
        # self.embedding在初始化时明确指定了num_hiddens=24（嵌入维度），这是嵌入向量维度的 “源头”：
        
        #         中间层：* math.sqrt(self.num_hiddens) → 词嵌入缩放
        # 作用：调整词嵌入的数值范围，使其与位置编码的范围匹配，避免位置信息被淹没。
        # 核心原因：
        # 位置编码的数值范围固定在[-1, 1]（由正弦 / 余弦函数生成）；
        # 词嵌入默认初始化的数值范围是[-1/√num_hiddens, 1/√num_hiddens]（如 24 维时约[-0.204, 0.204]），直接相加会导致位置信息被词嵌入 “覆盖”；
        # 乘以√num_hiddens（如√24≈4.9）后，词嵌入的数值范围被放大到[-1, 1]级别，与位置编码匹配。
        
        # nn.Embedding 默认用「均匀分布」初始化权重矩阵（范围：[-1/√embedding_dim, 1/√embedding_dim]），
        # 因此每个索引的 24 维向量是随机的、独立的浮点数，与索引值无关。
        
        # *math.sqrt(self.num_hiddens)：缩放 → 数值范围调整，形状不变；
        # self.pos_encoding(...)：添加位置编码 → 形状仍为(2,100,24)（位置编码与词嵌入逐元素相加）。
        
        self.attention_weights = [None]*len(self.blks)
        for i,blk in enumerate(self.blks):
            X = blk(X,valid_lens)
            self.attention_weights[i]=blk.attention.attention.attention_weights
        return X
```


```python
encoder = TransformerEncoder(
    200,24,24,24,24,[100,24],24,48,8,2,0.5)
encoder.eval()
encoder(torch.ones((2,100),dtype=torch.long),valid_lens).shape
```




    torch.Size([2, 100, 24])




```python

```
