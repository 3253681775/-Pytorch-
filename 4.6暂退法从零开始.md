```python
import torch 
from torch import nn
from d2l import torch as d2l
```


```python
def dropout_layer(X,dropout):
    assert 0 <= dropout <=1
    #使用断言确保dropout参数在合理范围内（0 到 1 之间），如果超出这个范围会直接报错，避免不合理的参数值。
    #在本情况下所有的元素都要杯丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    #在这种情况下所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape)>dropout).float()
    #torch.rand(X.shape)生成一个与X形状相同的张量，元素值在 [0,1) 区间均匀分布
    #> dropout进行比较操作，生成布尔张量（大于 dropout 的位置为 True，否则为 False）
    #.float()将布尔张量转换为浮点张量（True 变为 1.0，False 变为 0.0）
    #最终得到的mask张量中，约有(1-dropout)比例的元素为 1.0，dropout比例的元素为 0.0
    return mask*X/(1.0-dropout)
    #mask * X：通过逐元素相乘，将mask中为 0.0 位置对应的X元素丢弃（置为 0）
    #/ (1.0 - dropout)：对保留下来的元素进行缩放，确保 dropout 前后的期望值不变
```


```python
X = torch.arange(16,dtype=torch.float32).reshape((2,8))
print(X)
print(dropout_layer(X,0.))
print(dropout_layer(X,0.5))
print(dropout_layer(X,1.))
```

    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
    tensor([[ 0.,  0.,  4.,  6.,  0., 10., 12.,  0.],
            [16.,  0.,  0., 22., 24., 26.,  0., 30.]])
    tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0.]])
    


```python
#定义模型参数
num_inputs,num_outputs,num_hiddens1,num_hiddens2 = 784,10,256,256
```


```python
#定义模型
dropout1,dropout2 = 0.2,0.5
class Net(nn.Module):
    def __init__(self,num_inputs,num_outputs,num_hiddens1,num_hiddens2,is_training=True):
        super(Net,self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs,num_hiddens1)
        #定义第一个全连接层：将输入特征映射到第一个隐藏层
        #输入维度：num_inputs
        #输出维度：num_hiddens1
        self.lin2 = nn.Linear(num_hiddens1,num_hiddens2)
        #定义第二个全连接层：将第一个隐藏层映射到第二个隐藏层
        #输入维度：num_hiddens1
        #输出维度：num_hiddens2
        self.lin3 = nn.Linear(num_hiddens2,num_outputs)
        #定义第三个全连接层（输出层）：将第二个隐藏层映射到最终输出
        #输入维度：num_hiddens2
        #输出维度：num_outputs
        self.relu = nn.ReLU()

    def forward(self,X):
        #定义前向传播方法，描述数据在网络中的流动过程
        
        H1 = self.relu(self.lin1(X.reshape((-1,self.num_inputs))))
        #X.reshape((-1, self.num_inputs))：将输入数据重塑为二维张量（样本数 × 特征数）
        #self.lin1(...)：通过第一个全连接层
        #self.relu(...)：应用 ReLU 激活函数
        #结果存储在H1（第一个隐藏层输出）
        
        #只有在训练模型时才使用暂退法
        if self.training == True:
            #在第一个全连接层之后添加一个暂退层
            H1  = dropout_layer(H1,dropout1)
            
        H2 = self.relu(self.lin2(H1))
        #self.lin2(H1)：将第一个隐藏层输出通过第二个全连接层
        #self.relu(...)：应用 ReLU 激活函数
        #结果存储在H2（第二个隐藏层输出）
        
        if self.training == True:
            #在第二个全连接层之后添加一个暂退层
            H2 = dropout_layer(H2,dropout2)
            
        out = self.lin3(H2)
        return out

net = Net(num_inputs,num_outputs,num_hiddens1,num_hiddens2)
```


```python
#训练和测试
num_epochs,lr,batch_size = 10,0.5,256
loss =nn.CrossEntropyLoss(reduction='none')
#使用交叉熵损失（CrossEntropyLoss），适用于分类任务
#reduction='none'表示不自动对批次内的损失值进行聚合（不计算均值或总和）


train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(),lr=lr)
d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)
#注意：D2L 的train_ch3函数会自行处理损失的聚合方式
```


    
![svg](output_5_0.svg)
    



```python
#简洁实现
net = nn.Sequential(nn.Flatten(),  # 将输入的二维图像(如28×28)展平为一维向量(784个元素)
                    nn.Linear(784,256),
                    #实现全连接层（线性变换），计算公式为：y = x·W + b，其中W是权重矩阵，b是偏置。
                    #nn.Linear(in_features, out_features, bias=True)
                    #in_features：输入特征数（输入张量最后一维的大小）
                    #out_features：输出特征数（输出张量最后一维的大小）
                    #bias：是否添加偏置项（默认True）
                    nn.ReLU(),#计算公式为：ReLU(x) = max(0, x)
                    #在第一个全连接层之后添加一个暂退层
                    nn.Dropout(dropout1),#训练时随机将部分神经元的输出置为 0（按概率p），用于防止过拟合。
                    nn.Linear(256,256),
                    nn.ReLU(),
                    #在第二个全连接层之后添加一个暂退层
                    nn.Dropout(dropout2),
                    nn.Linear(256,10))
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight,std=0.01)
#定义权重初始化函数init_weights
#逻辑：遍历模型中的所有模块，当模块是全连接层 (nn.Linear) 时：
#使用nn.init.normal_进行初始化
#权重服从均值为 0、标准差为 0.01 的正态分布
#偏置项默认初始化为 0，这里没有特别设置

net.apply(init_weights);
#对模型中的所有层应用init_weights初始化函数
#apply方法会递归地对模型中的所有子模块执行传入的函数
#分号;用于抑制输出 (在 Jupyter 环境中常用)
```


```python
trainer = torch.optim.SGD(net.parameters(),lr = lr)
#创建优化器：使用随机梯度下降 (SGD)
#net.parameters()：获取模型中所有可训练的参数
#lr=lr：设置学习率为之前定义的 0.5

d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)
#net：要训练的神经网络模型
#train_iter：训练数据迭代器
#test_iter：测试数据迭代器
#loss：损失函数 (之前定义的交叉熵损失)
#num_epochs：训练轮次 (10 轮)
#trainer：优化器 (这里是 SGD)
```


    
![svg](output_7_0.svg)
    



```python

```
