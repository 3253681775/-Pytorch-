```python
import torch
import torch.nn.functional as F
from torch import nn

#不带参数的层
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self,X):
        return X - X.mean()

layer = CenteredLayer()
layer(torch.FloatTensor([1,2,3,4,5]))
```




    tensor([-2., -1.,  0.,  1.,  2.])




```python
net = nn.Sequential(nn.Linear(8,128),CenteredLayer())
Y = net(torch.rand(4,8))
Y.mean()
```




    tensor(3.7253e-09, grad_fn=<MeanBackward0>)




```python
#带参数的层
class MyLinear(nn.Module):
    def __init__(self,in_units,units):
        super().__init__()
        
        self.weight = nn.Parameter(torch.randn(in_units,units))
        
        #torch.randn(in_units, units)：生成一个形状为 (in_units, units) 的随机张量（服从标准正态分布 N (0,1)）。
        #nn.Parameter(...)：将张量包装为 nn.Parameter 类型 —— 这是 PyTorch 中标记 “可训练参数” 的关键，
        #会被自动纳入模型的参数管理（如 parameters() 方法可获取，反向传播时会计算梯度）。
        
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self,X):
        linear = torch.matmul(X,self.weight.data) + self.bias.data
        return F.relu(linear)
```


```python
linear = MyLinear(5,3)
linear.weight
```




    Parameter containing:
    tensor([[ 0.2929, -1.6727, -0.6144],
            [-0.5777,  0.2632,  0.9399],
            [-0.1871,  0.2959,  0.1668],
            [-0.3288,  1.1726,  0.3950],
            [-0.0153, -1.1276,  1.1114]], requires_grad=True)




```python
linear(torch.rand(2,5))
```




    tensor([[0.0000, 0.0000, 1.5622],
            [0.0000, 0.8590, 1.5535]])




```python
net = nn.Sequential(MyLinear(64,8),MyLinear(8,1))
net(torch.rand(2,64))
```




    tensor([[0.],
            [0.]])




```python

```
