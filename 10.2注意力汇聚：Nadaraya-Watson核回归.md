```python
import torch 
from torch import nn
from d2l import torch as d2l
```


```python
#生成数据集
n_train = 50 #训练样本数
x_train,_ = torch.sort(torch.rand(n_train)*5)#排序后的训练样本
def f(x):
    return 2*torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0,0.5,(n_train,))#训练样本的输出
x_test = torch.arange(0,5,0.1)#测试样本
y_truth = f(x_test)#测试样本的真实输出
n_test = len(x_test)#测试样本数
n_test
```




    50




```python
def plot_kernel_reg(y_hat):
    d2l.plot(x_test,[y_truth,y_hat],'x','y',legend=['Truth','Pred'],
             xlim=[0,5],ylim=[-1,5])
    #核心：调用d2l.plot绘制测试数据的真实值与预测值曲线（D2L 封装的高阶绘图函数），参数详解：
# x_test：X 轴数据，即测试特征（0-5 的等间隔数）；
# [y_truth,y_hat]：Y 轴数据，是一个列表，包含真实值y_truth和预测值y_hat，会自动绘制两条曲线；
# 'x'/'y'：X 轴和 Y 轴的标签；
# legend=['Truth','Pred']：图例，分别对应真实值和预测值曲线；
# xlim=[0,5]/ylim=[-1,5]：X 轴和 Y 轴的显示范围，固定视野方便对比；
# 该函数会自动完成画布大小、网格、曲线样式等配置，无需手动设置。
    d2l.plt.plot(x_train,y_train,'o',alpha=0.5);
    #补充：调用d2l.plt.plot绘制训练数据点（原生 Matplotlib 的散点绘制），参数详解：
# x_train/y_train：训练数据的特征和标签，作为散点的坐标；
# 'o'：指定标记样式为圆形（表示散点）；
# alpha=0.5：设置散点的透明度为 50%，避免点重叠导致的视觉遮挡；
# 分号;：在 Jupyter Notebook 中抑制不必要的对象输出；
# 这里用d2l.plt.plot而非d2l.plot，是因为需要自定义散点样式，原生接口更灵活
```


```python
#平均汇聚层
y_hat = torch.repeat_interleave(y_train.mean(),n_test)
plot_kernel_reg(y_hat)
```


    
![svg](output_3_0.svg)
    



```python
#非参数注意力汇聚
#X_repeat的形状为（n_test,n_train)
#每行都包含相同的测试输入(例如同样的查询)
X_repeat = x_test.repeat_interleave(n_train).reshape((-1,n_train))
#核心作用：将每个测试样本（查询）重复n_train次，构造一个(n_test, n_train)的矩阵，让每个查询能和所有训练键逐一计算相似度。
分步拆解：
# x_test.repeat_interleave(n_train)：对x_test中的每个元素重复n_train次。
# 例：若x_test = [a, b]（n_test=2），n_train=3，则结果为[a,a,a,b,b,b]。
# .reshape((-1, n_train))：将一维的重复结果重塑为二维矩阵，-1表示让 PyTorch 自动计算该维度的大小（结果为n_test）。
# 续上例：重塑后为[[a,a,a],[b,b,b]]，形状(2,3)，即(n_test, n_train)。
# 最终效果：X_repeat的每一行都是同一个测试输入重复n_train次，对应 “一个查询匹配所有训练键”。

#x_train包含键。attention_weights的形状为(n_test,n_train)
#每一行都包含要在给定的每个查询的值(y_train)之间分配注意力权重
attention_weights = nn.functional.softmax(-(X_repeat -x_train)**2 /2,dim=1)
#y_hat的每个元素都是值的加权平均值，期中的权重是注意力权重
y_hat = torch.matmul(attention_weights,y_train)
plot_kernel_reg(y_hat)
```


    
![svg](output_4_0.svg)
    



```python
d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0),
                  #原始张量维度：attention_weights → 形状(n_test, n_train)（2 维：查询数 × 键数）。
# 第一次unsqueeze(0)：在第 0 维插入一个维度，张量形状变为(1, n_test, n_train)（3 维）。
# unsqueeze(dim)：在指定维度插入一个大小为 1 的维度，不改变数据本身，仅调整形状。
# 这里的1对应批次数（表示当前只有 1 个批次的注意力权重）。
# 第二次unsqueeze(0)：再次在第 0 维插入一个维度，张量形状变为(1, 1, n_test, n_train)（4 维）。
# 这里的第二个1对应注意力头数（表示当前用了 1 个注意力头，没有多头注意力）。
                  xlabel='Sorted training inputs',
                  ylabel='Sorted testing inputs')
#d2l.show_heatmaps要求的输入维度是(batch_size, num_heads, num_queries, num_keys)，即：
# batch_size=1（批次数）
# num_heads=1（注意力头数）
# num_queries=n_test（查询数，对应测试输入数）
# num_keys=n_train（键数，对应训练输入数）
#代码中热力图的标签标注了Sorted training inputs/Sorted testing inputs（排序后的训练 / 测试输入），
#这是因为代码开头对x_train做了排序：x_train,_ = torch.sort(torch.rand(n_train)*5)，
#而x_test本身是 0 到 5 的有序序列。排序后，查询和键的数值大小是连续的，
#因此注意力权重的热力图会呈现出对角线集中的规律（数值相近的查询会关注数值相近的键），
#这也让 “查询数 / 键数” 对应的样本顺序更易解读。
```


    
![svg](output_5_0.svg)
    



```python

```
