```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```


```python
def masked_softmax(X,valid_lens):
    """通过在最后一个轴上掩蔽元素来执行softmax操作"""
    #X:3D张量，valid_lens:1D或2D张量
    if valid_lens is None:
        return nn.functional.softmax(X,dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens,shape[1])
            #核心目的：让valid_lens的长度 = 最小单元总数（batch_size * seq_len），
            #每个元素对应一个最小单元的有效长度。
            #
            #示例 1 适配过程（valid_lens=1D [2,3]）：
            # shape[1] = 2（seq_len=2）；
            # torch.repeat_interleave([2,3], 2) → 对每个元素重复 2 次 → valid_lens = [2,2,3,3]（长度 = 2*2=4，对应 4 个最小单元）；
            # 含义：4 个最小单元的有效长度分别为 [2,2,3,3]。
        
        else:
            valid_lens = valid_lens.reshape(-1)
#             #示例 2 适配过程（valid_lens=2D [[1,3],[2,4]]）：
            # reshape(-1) → 展平为 1D → valid_lens = [1,3,2,4]（长度 = 2*2=4，对应 4 个最小单元）；
            # 含义：4 个最小单元的有效长度分别为 [1,3,2,4]。
            
        #最后一个轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
        X = d2l.sequence_mask(X.reshape(-1,shape[-1]),valid_lens,value=-1e6)
        #shape[-1]：保留最后一个轴的长度
        # shape 是之前通过 shape = X.shape 获取的 X 原始形状（3D 张量，如 (batch_size, seq_len, dim)）；
        # shape[-1] 表示 “取形状元组的最后一个元素”，对应 X 的最后一个轴（dim 轴，即要做 softmax 和掩蔽的维度）；
        # 示例：若 X.shape=(2,2,4)（之前的调用示例），则 shape[-1] = 4，表示保留最后一个轴的长度为 4。
        # 2. -1：自动计算第一个轴的长度
        # reshape(a, b) 要求张量的总元素个数 = a * b（多维同理）；
        # 当其中一个维度设为 -1 时，PyTorch 会自动根据 “总元素个数” 和 “其他指定维度” 计算该维度的大小，无需手动计算；
        # 核心逻辑：第一个轴的长度 = 总元素个数 // shape[-1]。
        # 3. 整体效果：3D → 2D 的展平规则
        # 对于原始 3D 张量 X.shape=(batch_size, seq_len, dim)：
        # 总元素个数 = batch_size * seq_len * dim；
        # 展平后 2D 张量的形状 = (batch_size * seq_len, dim)；
        # 本质：把 X 的前两个轴（batch_size 和 seq_len）“合并” 为一个轴，最后一个轴（dim）保持不变，形成 “每行对应一个最小单元（原 3D 中的 X[i][j]）” 的 2D 结构。
        
        return nn.functional.softmax(X.reshape(shape),dim=-1)
        #X.reshape(shape)：将掩蔽后的 2D 张量恢复为原始 3D 形状（示例 1/2 中仍为(2,2,4)）；
        # softmax(..., dim=-1)：对最后一个轴（dim=2，4 维向量）做 softmax；
        # 效果：无效元素（-1e6）的指数为e^(-1e6)≈0，权重≈0；有效元素权重和 = 1。
```


```python
masked_softmax(torch.rand(2,2,4),torch.tensor([2,3]))
#输入X.shape=(2,2,4)：batch_size=2，seq_len=2，dim=4（每个最小单元是 4 维向量）；
# 输入valid_lens=torch.tensor([2,3])：1D 张量（batch_size=2），
#含义是 “第 1 个 batch 的所有最小单元有效长度 = 2，第 2 个 batch 的所有最小单元有效长度 = 3”。
```




    tensor([[[0.4584, 0.5416, 0.0000, 0.0000],
             [0.5531, 0.4469, 0.0000, 0.0000]],
    
            [[0.1931, 0.4593, 0.3477, 0.0000],
             [0.2923, 0.4016, 0.3060, 0.0000]]])




```python
masked_softmax(torch.rand(2,2,4),torch.tensor([[1,3],[2,4]]))
#输入X.shape=(2,2,4)：同示例 1；
# 输入valid_lens=torch.tensor([[1,3],[2,4]])：2D 张量（batch_size=2, seq_len=2），
#含义是 “第 1 个 batch 的第 1 个序列有效长度 = 1、第 2 个序列有效长度 = 3；第 2 个 batch 的第 1 个序列有效长度 = 2、第 2 个序列有效长度 = 4”。
```




    tensor([[[1.0000, 0.0000, 0.0000, 0.0000],
             [0.4434, 0.3479, 0.2087, 0.0000]],
    
            [[0.4583, 0.5417, 0.0000, 0.0000],
             [0.3170, 0.2134, 0.2568, 0.2128]]])




```python
#加性注意力
class AdditiveAttention(nn.Module):
    """加性注意力"""
    def __init__(self,key_size,query_size,num_hiddens,dropout,**kwargs):
        super(AdditiveAttention,self).__init__(**kwargs)

        ## 1. 键（Key）的线性变换层：将key从key_size维映射到num_hiddens维，无偏置
        self.W_k = nn.Linear(key_size,num_hiddens,bias=False)

        # # 2. 查询（Query）的线性变换层：将query从query_size维映射到num_hiddens维，无偏置
        self.W_q = nn.Linear(query_size,num_hiddens,bias=False)

        ## 3. 评分函数的输出层：将num_hiddens维特征映射到1维（相似度分数），无偏置
        self.w_v = nn.Linear(num_hiddens,1,bias=False)

        ## 4. Dropout层：防止过拟合，dropout rate由参数指定
        self.dropout = nn.Dropout(dropout)

        #定义 3 个可学习线性层，将 query 和 key 映射到同一维度（num_hiddens），
        #为后续计算相似度做准备（加性注意力的核心是 “先映射再相加”）。

    def forward(self,queries,keys,values,valid_lens):

        ## 步骤1：对查询和键做线性变换（映射到num_hiddens维）
        queries,keys = self.W_q(queries),self.W_k(keys)
        
        #在维度拓展后
        #queries的形状为(batch_size,查询数，1，num_hidden)
        #key的形状为(batch_size,1,键值对数,num_hiddens)
        # # 形状变化（结合示例）：
    # queries输入形状：(2,1,20) → 经W_q(20→8) → (2,1,8)（batch_size=2，查询数=1，num_hiddens=8）
    # keys输入形状：(2,10,2) → 经W_k(2→8) → (2,10,8)（batch_size=2，键值对数=10，num_hiddens=8）
        
        #使用广播方式求和
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        ## 步骤2：维度拓展，用广播机制实现“每个查询与所有键配对”
        # queries.unsqueeze(2)：在第2维（查询数之后）加一个维度 → (2,1,1,8)
        # keys.unsqueeze(1)：在第1维（键值对数之前）加一个维度 → (2,1,10,8)
        # 广播求和：(2,1,1,8) + (2,1,10,8) → (2,1,10,8)（每个查询与10个键逐一配对后的特征）
        #广播后的特征张量 features.shape=(2,1,10,8)，四个维度的含义依次是：
        # 维度索引	维度大小	核心含义	对应内容
        # dim=0	2	batch_size（批量大小）	2 个独立的样本批次
        # dim=1	1	query_num（查询个数）	每个批次只有 1 个查询
        # dim=2	10	key_num（键的个数）	每个批次有 10 个键
        # dim=3	8	hidden_dim（隐藏特征维度）	每个查询 / 键的特征维度（经线性映射后

        # 步骤3：非线性激活（tanh），增强模型表达能力
        features = torch.tanh(features)# 形状不变：(2,1,10,8)

        #  # 步骤4：计算相似度分数（scores
        #self.w_v仅有一个输出，因此从形状中移除最后那个维度
        #scores的形状为(batch_size,查询数,键值对数)
        # # self.w_v(features)：将(2,1,10,8) → (2,1,10,1)（每个(query-key)对的分数）
        # squeeze(-1)：移除最后一个维度（1维）→ (2,1,10)（batch_size=2，查询数=1，键值对数=10）
        scores = self.w_v(features).squeeze(-1)

        # 步骤5：计算带掩码的注意力权重（无效键的权重设为0）
        self.attention_weights = masked_softmax(scores,valid_lens)
        # # 形状：(2,1,10)（每行是一个查询对所有键的权重，和为1）
        
        #values的形状为(batchsize_size，键值对数，值的维度)
        return torch.bmm(self.dropout(self.attention_weights),values)
        # # 步骤6：Dropout正则化（随机丢弃部分权重）
        # 步骤7：加权聚合值（Values）→ 矩阵乘法：(batch, 查询数, 键数) × (batch, 键数, 值维度) → (batch, 查询数, 值维度)
        # values形状：(2,10,4) → 输出形状：(2,1,4)
         # 含义：每个查询的输出 = 注意力权重 × 对应Value的加权和
                
        #torch.bmm 是 PyTorch 中专门用于 3D 批量矩阵乘法 的函数，核心规则：
        # 输入必须是两个 3D 张量，且前两个维度（batch_size）必须完全一致；
        # 矩阵乘法仅作用于最后两个维度，即：
        # 若输入 A 形状为 (B, M, K)，输入 B 形状为 (B, K, N)，则输出形状为 (B, M, N)；
        # 每个批次独立计算矩阵乘法，批次间互不干扰（这也是 “批量” 的含义）。
#         输入 A（权重）：(2, 1, 10) → B=2，M=1，K=10；
        # 输入 B（值）：(2, 10, 4) → B=2，K=10，N=4；
        # 输出形状：(2, 1, 4) → B=2，M=1，N=4（与预期一致）
```


```python
queries,keys = torch.normal(0,1,(2,1,20)),torch.ones((2,10,2))
# 1. 构造测试数据：查询（Queries）和键（Keys）
# queries：(batch_size=2, 查询数=1, query_size=20) → 2个batch，每个batch 1个查询，每个查询20维
# torch.normal(0,1,...)：生成均值0、方差1的随机张量
# keys：(batch_size=2, 键值对数=10, key_size=2) → 2个batch，每个batch 10个键，每个键2维（全1张量）

# 2. 构造测试数据：值（Values）
# values：(1,10,4) → 先创建0-39的连续值，reshape为(1,10,4)（1个batch，10个值，每个值4维）
# repeat(2,1,1)：复制为2个batch → 最终形状(2,10,4)（两个batch的values完全相同）
#values的小批量，两个值矩阵是相同的
values = torch.arange(40,dtype=torch.float32).reshape(1,10,4).repeat(2,1,1)

# 3. 构造有效长度（valid_lens）：(2,) → 1D张量，指定每个batch的有效键数
# 第1个batch：前2个键有效，后8个无效；第2个batch：前6个键有效，后4个无效
valid_lens = torch.tensor([2,6])

## 4. 实例化加性注意力模型
# key_size=2（键的维度），query_size=20（查询的维度），num_hiddens=8（隐藏层维度），dropout=0.1（丢弃概率）
attention = AdditiveAttention(key_size=2,query_size=20,num_hiddens=8,dropout=0.1)

# 5. 切换模型为评估模式（eval()）：禁用Dropout等训练时的功能，仅用于推理
attention.eval()

# 6. 执行前向传播，得到注意力聚合结果
# 输入：queries(2,1,20)、keys(2,10,2)、values(2,10,4)、valid_lens(2,)
# 输出：(2,1,4) → 2个batch，每个batch 1个查询的聚合结果（4维）
attention(queries,keys,values,valid_lens)
```




    tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],
    
            [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)




```python
# values是0~39的固定连续值，按(10,4)排列，且两个 batch 完全相同：
# 第 1 个 batch 有效键是前 2 个：values[0][0] = [0,1,2,3]，values[0][1] = [4,5,6,7]；
# 若权重平均分配（0.5 和 0.5），则输出 = 0.5×[0,1,2,3] + 0.5×[4,5,6,7] = [2.0,3.0,4.0,5.0]（和你看到的一致）；
# 第 2 个 batch 有效键是前 6 个：values[0][0~5] = [0,1,2,3], [4,5,6,7], [8,9,10,11], [12,13,14,15], [16,17,18,19], [20,21,22,23]；
# 若权重平均分配（每个≈0.1667），则输出 = 平均求和 = (0+4+8+12+16+20)/6=10.0，(1+5+9+13+17+21)/6=11.0，以此类推，刚好是[10.0,11.0,12.0,13.0]

# 四、核心原因 3：queries的随机性被 “模型结构” 大幅削弱
# 虽然queries = torch.normal(0,1,(2,1,20))是随机的，但加性注意力的结构会 “抹平” 这种随机性：
# queries经self.W_q变换后是 8 维向量，而keys变换后是 “全相同的 8 维向量”；
# 相似度分数scores = w_v(tanh(W_q q + W_k k))—— 因为W_k k对所有键都相同，所以W_q q + W_k k的差异仅来自W_q q（查询的变换结果），但tanh激活会把这个差异压缩到(-1,1)之间，再经w_v映射后，分数差异进一步缩小；
# 最终masked_softmax后，有效键的权重仍近似平均分配，queries的随机性仅导致权重有 ±0.05 的微小波动，反映到输出上就是 ±0.5 的波动，不会偏离[2.0±0.5, 3.0±0.5,...]和[10.0±0.5,...]的范围。
```


```python
d2l.show_heatmaps(attention.attention_weights.reshape((1,1,2,10)),xlabel='Keys',ylabel='Queries')
## 按你指定的维度重塑（batch=1, head=1, queries=2, keys=10）
#
```


    
![svg](output_7_0.svg)
    



```python
#10.3.3缩放点积注意力
class DotProductAttention(nn.Module):
    """缩放点积注意力"""
    def __init__(self,dropout,**kwargs):
        super(DotProductAttention,self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.scores = None  # 保存缩放后分数
        self.attention_weights = None  # 初始化权重为None
    #queries的形状为(batch_size,查询数，d)
    #keys的形状为(batchsize,键值对数，d)
    #values的形状为(batchsize,键值对数，值的维度)
    #valid_lens的形状为(batch_size,)或者（batch_size,查询数)
    def forward(self,queries,keys,values,valid_lens=None):
        d = queries.shape[-1]
        
        #设置transpose_b = True是为了交换keys的最后两个角度
        self.scores = torch.bmm(queries,keys.transpose(1,2))/math.sqrt(d)
        # 步骤1：计算点积分数 + 缩放（除以√d，避免维度d过大导致分数爆炸）
        # keys.transpose(1,2)：键的形状从(batch, 键数, d) → (batch, d, 键数)
        # torch.bmm：批量矩阵乘法 → (batch, 查询数, d) × (batch, d, 键数) → (batch, 查询数, 键数)
        # 除以math.sqrt(d)：缩放因子，核心区别于普通点积注意力
        
        #点积注意力的分数计算公式是 score(q,k) = q·k（q 是查询向量，k 是键向量，维度均为 d）。假设 q 和 k 的元素都是独立同分布的随机变量（均值 0，方差 1），则：
        # 点积q·k = q₁k₁ + q₂k₂ + ... + q_dk_d；
        # 根据概率论，点积的方差 = d × (q 的方差 × k 的方差) = d×1×1 = d；
        # 点积的均值 = 0（因为 q/k 元素均值为 0）。
        # 这意味着：维度 d 越大，点积的数值范围就越大（比如 d=512 时，点积的方差 = 512，数值可能达到 ± 几十）。
        # 而 Softmax 函数的特性是：输入值越大，Softmax 的输出越趋近于 1（或 0），梯度越接近 0（梯度消失）。

         # 步骤2：带掩码的Softmax → 得到注意力权重
        self.attention_weights = masked_softmax(self.scores,valid_lens)

        # 步骤3：Dropout + 加权聚合Values
        return torch.bmm(self.dropout(self.attention_weights),values)
```


```python
# 1. 构造测试数据（关键：查询/键维度必须相等，这里设为2）
# queries：(batch=2, 查询数=1, d=2) → 维度d=2，和keys的最后一维一致
queries = torch.normal(0,1,(2,1,2))

# keys：(batch=2, 键数=10, d=2) → 维度d=2，和queries匹配
keys = torch.ones((2, 10, 2))

# values：(batch=2, 键数=10, value_dim=4) → 和加性注意力保持一致
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)

# valid_lens：掩码规则 → 第1个batch前2个键有效，第2个前6个有效
valid_lens = torch.tensor([2, 6])

# 2. 实例化模型 + 评估模式
attention = DotProductAttention(dropout=0.5)
# attention.eval()
# 你的DotProductAttention包含nn.Dropout(dropout)层，这是训练时防止过拟合的组件，推理 / 测试时必须禁用：
# 训练模式（默认，self.training = True）：Dropout 会随机丢弃部分注意力权重（按 dropout=0.5 的概率）；
# 推理模式（eval()后，self.training = False）：Dropout 层会原样输出所有权重，不做任何丢弃 → 保证注意力权重的计算结果稳定、可复现。
# 若跳过eval()，直接调用attention(queries, keys, values, valid_lens)，Dropout 会随机丢弃权重，导致每次运行的注意力权重 / 输出结果不一致，无法得到稳定的测试 / 可视化结果。
attention.eval()

output = attention(queries, keys, values, valid_lens)

# 3. 提取 scores 与权重（detach 脱离计算图）
scores = attention.scores.detach()  # (2, 1, 10)
weights = attention.attention_weights.detach()  # (2, 1, 10)

# 打印形状与数值
print("scores shape:", scores.shape)
print("scores:\n", scores)
print("attention_weights shape:", weights.shape)
print("attention_weights:\n", weights)
```

    scores shape: torch.Size([2, 1, 10])
    scores:
     tensor([[[-7.1583e-02, -7.1583e-02, -1.0000e+06, -1.0000e+06, -1.0000e+06,
              -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]],
    
            [[ 1.9095e+00,  1.9095e+00,  1.9095e+00,  1.9095e+00,  1.9095e+00,
               1.9095e+00, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]]])
    attention_weights shape: torch.Size([2, 1, 10])
    attention_weights:
     tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
              0.0000, 0.0000]],
    
            [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,
              0.0000, 0.0000]]])
    


```python
d2l.show_heatmaps(attention.attention_weights.reshape((1,1,2,10)),xlabel='Keys',ylabel='Queries')
```


    
![svg](output_10_0.svg)
    



```python

```
