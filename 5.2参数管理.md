```python
#单隐藏层的多层感知机
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
X = torch.rand(size = (2,4))
net(X)
```




    tensor([[-0.0487],
            [-0.0649]], grad_fn=<AddmmBackward0>)




```python
#参数访问
print(net[2].state_dict())
#个层（如 nn.Linear、nn.Conv2d）作为 nn.Module 的子类，
#其内部参数（权重、偏置等）是以张量（torch.Tensor） 的形式直接存储在层的实例变量中的

#当我们调用模型的 state_dict() 方法时，PyTorch 会递归遍历模型中所有的层和子模块，
#将所有可训练参数（requires_grad=True）和缓冲区（如批归一化的 running_mean）收集到一个 OrderedDict 中。
#这个字典的键（key）是参数的 “路径名”（反映参数所属的层 / 块位置），值（value）是对应的参数张量
```

    OrderedDict([('weight', tensor([[-0.0020,  0.0986,  0.1486,  0.1319, -0.1009,  0.2191, -0.1350,  0.1825]])), ('bias', tensor([-0.1049]))])
    


```python
#目标参数
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
```

    <class 'torch.nn.parameter.Parameter'>
    Parameter containing:
    tensor([-0.1049], requires_grad=True)
    tensor([-0.1049])
    


```python
#还可以访问参数的梯度
net[2].weight.grad == None
```




    True




```python
#一次性访问所有的参数
print(*[(name,param.shape) for name,param in net[0].named_parameters()])
print(*[(name,param.shape) for name,param in net.named_parameters()])
#named_parameters() 是 nn.Module 类的方法，返回一个生成器，
#迭代产生模型中所有可训练参数的 (名称，参数张量) 键值对。

#* 在打印时用于解包迭代器，将列表中的元素逐个作为参数传入 print()，避免打印出列表的括号和逗号。

#列表推导式，将迭代器中的每个元素转换为 (参数名称, 参数形状) 的元组。
#param.shape 用于获取参数张量的形状（如 (20, 10) 表示权重矩阵的维度）
#解包列表，将列表中的每个元组作为独立参数传给 print()
```

    ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
    ('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
    


```python
net.state_dict()['2.bias'].data
#net.state_dict()：调用模型的 state_dict() 方法，返回一个包含模型所有可训练参数和缓冲区的有序字典（OrderedDict）。字典的键是参数名称（如 '2.bias'），值是对应的参数张量（torch.Tensor）。
#['2.bias']：通过字典的键 '2.bias' 索引，获取该键对应的值 —— 即模型中索引为 2 的模块的偏置参数张量。
#这里的 '2' 表示该参数所属模块在模型中的索引（例如 nn.Sequential 中第 3 个模块，索引从 0 开始）。
#'bias' 表示该参数是模块的偏置（对应的权重参数通常用 'weight' 表示）。
#.data：访问张量的 .data 属性，获取该参数张量不含梯度信息的底层数据（返回一个新的张量，与原张量共享数据但不关联梯度计算图
#.data 返回一个新的张量，该张量与原张量共享数据内存，但不包含任何梯度信息（requires_grad=False），也不与计算图关联。

#简单说：.data 相当于 “只取张量的值，丢掉它的梯度和计算图关系”。
```




    tensor([-0.1049])




```python
#从嵌套块收集参数
def block1():
    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),
                         nn.Linear(8,4),nn.ReLU())
def block2():
    net = nn.Sequential()
    for i in range(4):
        #在这里嵌套
        net.add_module(f'block{i}',block1())
        #add_module(f'block{i}', block1())：
        # 第一个参数 f'block{i}' 是子块的名称（如 block0、block1 等，便于索引和调试）。
        # 第二个参数 block1() 是调用 block1 函数创建的子块（即包含 2 个线性层 + 2 个 ReLU 的结构）。
    return net

rgnet = nn.Sequential(block2(),nn.Linear(4,1))
rgnet(X)
```




    tensor([[-0.2927],
            [-0.2927]], grad_fn=<AddmmBackward0>)




```python
print(rgnet)
```

    Sequential(
      (0): Sequential(
        (block0): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
        (block1): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
        (block2): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
        (block3): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
      )
      (1): Linear(in_features=4, out_features=1, bias=True)
    )
    


```python
rgnet[0][1][0].bias.data
```




    tensor([ 0.2925, -0.0258,  0.1540, -0.1408, -0.3162, -0.2335, -0.1134, -0.3825])




```python
#参数初始化
#内置初始化
def init_normal(m):
    if type(m) == nn.Linear:
        
        nn.init.normal_(m.weight,mean=0,std=0.01)
        #使用 nn.init.normal_ 函数对线性层的权重 m.weight 进行初始化：
        # normal_ 表示 “正态分布初始化”，后缀 _ 表示该函数会原地修改张量（直接修改 m.weight 的值）。
        
        nn.init.zeros_(m.bias)
        #使用 nn.init.zeros_ 函数对线性层的偏置 m.bias 进行初始化，将其所有元素设为 0。

net.apply(init_normal)
    #net 是一个神经网络模型（如 nn.Sequential 实例）。
    # apply(init_normal) 会递归遍历 net 中所有的子模块（包括嵌套的层和块），并对每个子模块调用 init_normal 函数。
    # 最终效果：net 中所有的线性层（nn.Linear）的权重被初始化为 N(0, 0.01²)，偏置被初始化为 0。

net[0].weight.data[0],net[0].bias.data[0]
```




    (tensor([-0.0183, -0.0011,  0.0050, -0.0007]), tensor(0.))




```python
def init_constant(m):
    if type(m) == nn.Linear:
        
        nn.init.constant_(m.weight,1)
        #使用 nn.init.constant_ 函数将线性层的权重 m.weight 的所有元素设为常数 1（constant_ 表示 “常数初始化”，原地修改张量）。
        
        nn.init.zeros_(m.bias)

net.apply(init_constant)
net[0].weight.data[0],net[0].bias.data[0]
```




    (tensor([1., 1., 1., 1.]), tensor(0.))




```python
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
        #nn.init.xavier_uniform_ 是 PyTorch 内置的初始化函数，专为解决前向传播和反向传播中的梯度消失 / 爆炸问题设计，适用于激活函数为 tanh、sigmoid 等的场景。
        # 其原理是：使权重值服从均匀分布，且分布范围与输入、输出维度相关（保证前向和反向的信号方差一致）。
        # 后缀 _ 表示该函数原地修改权重张量（直接更新 m.weight 的值）

def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight,42)

net[0].apply(init_xavier)
net[2].apply(init_42)

print(net[0].weight.data[0])
#打印 net[0] 模块中权重张量的第 0 行数据，验证 Xavier 初始化的效果。
#输出应为服从 Xavier 均匀分布的数值（范围与输入输出维度相关）

print(net[2].weight.data)
```

    tensor([-0.4657, -0.5646,  0.2996, -0.2754])
    tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])
    


```python
#自定义初始化
def my_init(m):
    if type(m) == nn.Linear:
        print("Init",*[(name,param.shape)
                       for name,param in m.named_parameters()][0])
        nn.init.uniform_(m.weight,-10,10)
        m.weight.data *= m.weight.data.abs() >= 5
```


```python
net.apply(my_init)
net[0].weight[:2]
```

    Init weight torch.Size([8, 4])
    Init weight torch.Size([1, 8])
    




    tensor([[-9.3395, -0.0000,  0.0000,  6.9194],
            [ 0.0000,  0.0000,  9.6765, -7.2339]], grad_fn=<SliceBackward0>)




```python
#可以自己直接设置参数
net[0].weight.data[:] += 1
net[0].weight.data[0,0] = 42
net[0].weight.data[0]

```




    tensor([42.0000,  1.0000,  1.0000,  7.9194])




```python
#参数绑定
#我们需要给共享层提供一个名称，以便可以引用它的参数
shared = nn.Linear(8,8)
net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),
                    shared,nn.ReLU(),
                    shared,nn.ReLU(),
                    nn.Linear(8,1))
net(X)
#检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0,0] = 100
#确保他们实际上是同一个对象，而不只有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])

```

    tensor([True, True, True, True, True, True, True, True])
    tensor([True, True, True, True, True, True, True, True])
    


```python

```
