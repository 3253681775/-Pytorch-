```python
%matplotlib inline
import torch
from d2l import torch as d2l

```


```python
#ReLU函数，修正线性单元
x = torch.arange(-8.0,8.0,0.1,requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(),y.detach(),'x','relu(x)',figsize=(5,2.5))
#x.detach() 和 y.detach() 用于从计算图中分离出张量，因为我们只是想绘制它们的值，不需要跟踪梯度
#当使用 d2l.plot() 绘制时，默认会将这些密集的点连接成连续的线条
```


    
![svg](output_1_0.svg)
    



```python
#relu函数的导数，在0点处的导数取0
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(),x.grad,'x','grad of relu',figsize=(5,2.5))
#x.grad 存储了 x 的梯度值，也就是 ReLU 函数在各点的导数值

#y 是之前计算得到的 ReLU 函数输出（y = torch.relu(x)）
#backward() 是 PyTorch 中用于反向传播计算梯度的方法
#torch.ones_like(x) 创建了一个与x形状相同、元素全为 1 的张量，作为反向传播的初始梯度（也称为 "梯度种子"）
#对于标量输出，我们通常不需要指定这个参数，但这里y是张量（向量），所以需要提供一个同形状的张量作为每个元素的梯度权重
#retain_graph=True 表示保留计算图，这样我们可以多次调用 backward () 而不会报错（如果后续还需要计算其他梯度的话）

#在本例中，y = torch.relu(x)，y与x形状相同（都是向量）。我们的目标是计算每个x_i对y_i的导数（即 ReLU 函数在每个点的导数）。
#torch.ones_like(x)创建了一个与x（和y）形状相同、元素全为 1 的张量，相当于为y的每个元素y_i赋予了梯度值1.0。
#此时反向传播计算的是：x.grad[i] = 对y_i求导（结果为1） × 对x_i求导（即ReLU在x_i处的导数）。
#由于y_i对自身的导数是 1，最终x.grad中存储的就是ReLU 函数在每个x_i处的真实导数值，这正是我们想要可视化的结果。
```


    
![svg](output_2_0.svg)
    



```python
#sigmoid函数
y = torch.sigmoid(x)
d2l.plot(x.detach(),y.detach(),'x','sigmoid(x)',figsize=(5,2.5))
```


    
![svg](output_3_0.svg)
    



```python
#sigmoid函数的导数
#清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(),x.grad,'x','grad of sigmoid',figsize=(5,2.5))
```


    
![svg](output_4_0.svg)
    



```python
#tanh函数
y = torch.tanh(x)
d2l.plot(x.detach(),y.detach(),'x','tanh(x)',figsize=(5,2.5))
```


    
![svg](output_5_0.svg)
    



```python
#tanh函数的导数
#清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(),x.grad,'x','grad of tanh',figsize=(5,2.5))#单位是英寸
```


    
![svg](output_6_0.svg)
    



```python

```
