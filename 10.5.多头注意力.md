```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```


```python
class MultiHeadAttention(nn.Module):
    """多头注意力"""
    def __init__(self,key_size,query_size,value_size,num_hiddens,
                 num_heads,dropout,bias=False,**kwargs):
        #key_size：键（key）的维度；# query_size：查询（query）的维度；
        #value_size：值（value）的维度；num_hiddens：注意力输出的总隐藏层维度；
        #num_heads：注意力头的数量；dropout：Dropout 概率（防止过拟合）；
        #bias：线性层是否使用偏置；**kwargs：接收父类 nn.Module 的额外参数。
        
        super(MultiHeadAttention,self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size,num_hiddens,bias=bias)
        self.W_k = nn.Linear(key_size,num_hiddens,bias=bias)
        self.W_v = nn.Linear(value_size,num_hiddens,bias=bias)
        self.W_o = nn.Linear(num_hiddens,num_hiddens,bias=bias)
#         定义 3 个线性变换层，分别对 query/key/value 做维度映射：
# W_q：将 query 从 query_size 映射到 num_hiddens；
# W_k：将 key 从 key_size 映射到 num_hiddens；
# W_v：将 value 从 value_size 映射到 num_hiddens；
# 目的：统一 query/key/value 的维度，同时将维度扩展到总隐藏层维度 num_hiddens（后续会拆分到多个注意力头）。
# 定义输出线性层 W_o：将多个注意力头拼接后的结果（维度 num_hiddens）再次映射到 num_hiddens，作为最终输出。
    
    def forward(self,queries,keys,values,valid_lens):
        #queries,keys,values的形状分别为(batchsize,查询数或者键值对数，num_hiddens)
        #valid_lens的形状为(batch_size,)或者(batch_size,查询数)
        #经过变换后，输出的queries,keys,values的形状为
        #（batch_size*num_heads,查询数或者键值对数，num_hiddens/num_heads)
        queries = transpose_qkv(self.W_q(queries),self.num_heads)
        #第一步：线性变换：self.W_q(queries) 将 queries 映射到 num_hiddens 维度，
        #形状变为 (batch_size, 查询数, num_hiddens)；keys/values 同理。

        #第二步：维度转置拆分（transpose_qkv 函数）：
        # 核心作用：将 num_hiddens 维度拆分为 num_heads 个独立的注意力头，每个头的维度为 num_hiddens/num_heads；
        # 形状变化示例（假设 batch_size=2，查询数 = 3，num_hiddens=8，num_heads=2）：
        # 变换前：(2, 3, 8)；
        # 变换后：(2*2, 3, 4)（batch_size*num_heads 作为新的批次维度，每个头独立计算注意力）；
        # transpose_qkv 是辅助函数，逻辑为：先将 num_hiddens 拆分为 num_heads × (num_hiddens/num_heads)，再将 num_heads 维度与 batch_size 合并，最终得到 (batch_size×num_heads, 序列长度, num_hiddens/num_heads)。
        keys = transpose_qkv(self.W_k(keys),self.num_heads)
        values = transpose_qkv(self.W_v(values),self.num_heads)

        if valid_lens is not None:
            #在轴0,将第一项(标量或者向量)复制num_heads次
            #然后如此复制第二项，以此类推
            valid_lens = torch.repeat_interleave(
                valid_lens,repeats=self.num_heads,dim=0)
        # 有效长度扩展：
        # valid_lens 原本是针对 batch_size 个样本的有效长度，而现在 queries/keys/values 的批次维度变为 batch_size×num_heads（每个头对应一个 “虚拟批次”）；
        # torch.repeat_interleave：将 valid_lens 的每个元素在维度 0 上复制 num_heads 次，使有效长度与新的批次维度匹配；
        # 示例：若 valid_lens = [3, 5]（batch_size=2），num_heads=2，则复制后为 [3,3,5,5]（对应 batch_size×num_heads=4 个虚拟批次）。
        
        #output的形状为(batch_size*num_heads,查询数，num_hiddens/num_heads)
        output = self.attention(queries,keys,values,valid_lens)
        # 输出：每个注意力头的注意力加权结果，形状 (batch_size×num_heads, 查询数, num_hiddens/num_heads)。
        
        #output_concat的形状为(batch_size,查询数，num_hiddens)
        output_concat = transpose_output(output,self.num_heads)

        #  维度还原拼接（transpose_output 函数）：
        # 与 transpose_qkv 反向操作：将 batch_size×num_heads 拆分为 batch_size 和 num_heads，再将 num_heads 维度与 num_hiddens/num_heads 拼接，恢复为 num_hiddens；
        # 形状变化示例：(2×2, 3, 4) → (2, 3, 8)；
        # 最终得到所有注意力头拼接后的结果 output_concat，维度为 (batch_size, 查询数, num_hiddens)。
        
        return self.W_o(output_concat)
#         为什么拼接后还要过线性层 W_o？（形状不变但作用关键）
# 虽然 W_o 的输入和输出形状都是 (batch_size, 查询数, num_hiddens)，但这一步线性变换不是多余的，核心目的是融合多个注意力头的信息、弥补拆分带来的表达能力损失，下面从 3 个维度拆解原因：
# 1. 先明确：“形状不变” 是表象，信息结构已发生根本变化
# 多头注意力的拆分 - 拼接过程：
# 拆分：将 num_hiddens 拆分为 num_heads × (num_hiddens/num_heads)，每个头独立计算注意力（相当于 “分治”）；
# 拼接：将 num_heads 个独立头的输出（每个头维度 num_hiddens/num_heads）拼接回 num_hiddens。
# 拼接后的张量本质是 num_heads 个 “子空间” 的简单拼接，而非 “融合”—— 每个头的信息仍处于独立的维度区间，没有跨头的信息交互。例如：num_hiddens=8，num_heads=2，拼接后前 4 维是头 1 的输出，后 4 维是头 2 的输出，二者完全独立。
# 2. W_o 的核心作用：融合多头信息 + 重构特征空间
# W_o 是一个全连接层（nn.Linear(num_hiddens, num_hiddens)），其权重矩阵形状为 (num_hiddens, num_hiddens)，本质是对拼接后的所有维度做线性组合：
# 数学表达：Output = Concat(Head1, Head2, ..., HeadN) × W_o^T + b（若有偏置）；
# 效果：让头 1 的第 i 维、头 2 的第 j 维、…、头 N 的第 k 维之间产生交互，将 “简单拼接的多头信息” 重构为 “融合后的统一特征”。
```


```python
def transpose_qkv(X,num_heads):
    """为了多注意力头的并行计算而变换形状"""
    #输入X的形状为(batch_size,查询数或者键值对数，num_hiddens)
    #输出X的形状为(batch_size,查询数或者键值对数，num_hiddens,num_hiddens/num_heads)
    X = X.reshape(X.shape[0],X.shape[1],num_heads,-1)

    #输出X的形状为(batchsize,num_head,查询数或者键值对数，num_hiddens/num_heads)
    X = X.permute(0,2,1,3)
    # 核心操作：维度重排（permute）
    # permute(0,2,1,3) 表示：将维度顺序从 (0,1,2,3) 改为 (0,2,1,3)；
    # 维度 0：batch_size（不变）；
    # 维度 2：num_heads（从原位置 2 移到位置 1）；
    # 维度 1：seq_len（从原位置 1 移到位置 2）；
    # 维度 3：d_k（不变）；
    # 执行后 X 形状：(2, 2, 3, 4)（batch_size, num_heads, seq_len, d_k）；
    # 目的：将 num_heads 维度提前，为后续 “合并 batch 和 num_heads” 做准备（让每个头成为独立的 “虚拟批次”）。
    
    #最终输出的形状为(batch_size*num_heads,查询数或者键值对数，num_hiddens/num_heads)
    return X.reshape(-1,X.shape[2],X.shape[3])
    #     核心操作：合并批次维度（reshape）
    # reshape 参数解析：
    # -1 → 自动计算：batch_size × num_heads = 2×2=4；
    # X.shape[2] → seq_len（3）；
    # X.shape[3] → d_k（4）；
    # 执行后返回形状：(4, 3, 4)（batch_size×num_heads, seq_len, d_k）；
    # 核心意义：将多个注意力头转换为 “虚拟批次”，使得 PyTorch 可以并行计算所有头的注意力（无需循环）。
    
    # transpose_qkv 完整形状变化链
    # (2, 3, 8) → reshape → (2, 3, 2, 4) → permute → (2, 2, 3, 4) → reshape → (4, 3, 4)

def transpose_output(X,num_heads):
    """反转transpose_qkv函数的操作"""
    X = X.reshape(-1,num_heads,X.shape[1],X.shape[2])
    X = X.permute(0,2,1,3)
    return X.reshape(X.shape[0],X.shape[1],-1)
```


```python
num_hiddens,num_heads = 100,5
attention = MultiHeadAttention(num_hiddens,num_hiddens,
                               num_hiddens,num_hiddens,num_heads,0.5)
attention.eval()
```




    MultiHeadAttention(
      (attention): DotProductAttention(
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (W_q): Linear(in_features=100, out_features=100, bias=False)
      (W_k): Linear(in_features=100, out_features=100, bias=False)
      (W_v): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=100, out_features=100, bias=False)
    )




```python
batch_size,num_queries = 2,4
num_kvpairs,valid_lens = 6,torch.tensor([3,2])
X = torch.ones((batch_size,num_queries,num_hiddens))
Y = torch.ones((batch_size,num_kvpairs,num_hiddens))
attention(X,Y,Y,valid_lens).shape
```




    torch.Size([2, 4, 100])




```python
#  关键参数梳理
# 参数	取值	含义
# num_hiddens	100	注意力总隐藏层维度
# num_heads	5	注意力头数
# batch_size	2	批次大小
# num_queries	4	查询数（如 4 个查询 token）
# num_kvpairs	6	键值对数量（如 6 个 key/value token）
# valid_lens	[3,2]	每个样本的有效长度（屏蔽后 3/2 个 token）

# 步骤 1：实例化多头注意力
# python
# 运行
# attention = MultiHeadAttention(num_hiddens,num_hiddens,num_hiddens,num_hiddens,num_heads,0.5)
# 入参对应：key_size=100、query_size=100、value_size=100、num_hiddens=100、num_heads=5、dropout=0.5；
# 每个注意力头的维度：d_k = num_hiddens / num_heads = 20。

# 步骤 2：输入张量形状
# X (queries)：(2, 4, 100) → 线性变换后仍为 (2, 4, 100)；
# Y (keys/values)：(2, 6, 100) → 线性变换后仍为 (2, 6, 100)。

# 步骤 3：transpose_qkv 维度拆分
# queries 拆分后：(2×5, 4, 20) = (10, 4, 20)；
# keys 拆分后：(2×5, 6, 20) = (10, 6, 20)；
# values 拆分后：(2×5, 6, 20) = (10, 6, 20)。

# 步骤 4：valid_lens 扩展
# valid_lens = [3,2] → 重复 5 次后 → [3,3,3,3,3,2,2,2,2,2]（长度 10），与拆分后的批次维度匹配。

# 步骤 5：点积注意力计算
# 输入：queries(10,4,20)、keys(10,6,20)、values(10,6,20)；
# 注意力分数形状：(10, 4, 6)（每个查询对每个键的分数）；
# 输出形状：(10, 4, 20)（加权求和 value 后的结果）。

# 步骤 6：transpose_output 维度还原
# (10, 4, 20) → 还原后 → (2, 4, 100)（拼接 5 个头的 20 维→100 维）。

# 步骤 7：W_o 线性变换
# 输入 (2,4,100) → 输出仍为 (2,4,100)（融合多头信息）。

# 注意力的 “分治” 体现：
# 例如 num_heads=5 时，会拆分为 5 个独立的注意力头，每个头计算 Q/K/V 的子集维度（100→20）；
# 头 1 可能聚焦 “语法关联”（如主谓搭配），头 2 可能聚焦 “语义关联”（如近义词），最终通过 W_o 融合所有头的聚焦结果，让注意力更全面。
```
