```python
import torch 
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
```


```python
class Inception(nn.Module):
    #c1--c4是每条路径的输出通道数
    def __init__(self,in_channels,c1,c2,c3,c4,**kwargs):
        super(Inception,self).__init__(**kwargs)
        #路径1，单1*1卷积层
        self.p1_1 = nn.Conv2d(in_channels,c1,kernel_size=1)
        #路径2，1*1卷积层后接3*3卷积层
        self.p2_1 = nn.Conv2d(in_channels,c2[0],kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0],c2[1],kernel_size=3,padding=1)
        #路径3，1*1卷积层后接5*5卷积层
        self.p3_1 = nn.Conv2d(in_channels,c3[0],kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0],c3[1],kernel_size=5,padding=2)
        #路径4，3*3最大汇聚层后接1*1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3,stride=1,padding=1)
        self.p4_2 = nn.Conv2d(in_channels,c4,kernel_size = 1)

    def forward(self,x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        #在通道维度上连接输出
        return torch.cat((p1,p2,p3,p4),dim = 1)
```


```python
b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size =7,stride=2,padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
```


```python
b2 = nn.Sequential(nn.Conv2d(64,64,kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64,192,kernel_size=3,padding=1),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
```


```python
b3 = nn.Sequential(Inception(192,64,(96,128),(16,32),32),
                   Inception(256,128,(128,192),(32,96),64),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
```


```python
b4 = nn.Sequential(Inception(480,192,(96,208),(16,48),64),
                   Inception(512,160,(112,224),(24,64),64),
                   Inception(512,128,(128,256),(24,64),64),
                   Inception(512,112,(144,288),(32,64),64),
                   Inception(528,256,(160,320),(32,128),128),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
```


```python
b5 = nn.Sequential(Inception(832,256,(160,320),(32,128),128),
                   Inception(832,384,(192,384),(48,128),128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())
net = nn.Sequential(b1,b2,b3,b4,b5,nn.Linear(1024,10))
#二、全局平均汇聚的工作原理（分步解析）
# 假设输入到全局平均汇聚层的特征图维度为 [N, C, H, W]：
# N：批量大小（代码中为 1）；
# C：通道数（代码中最后一个 Inception 模块输出为 1024）；
# H/W：特征图的空间高度 / 宽度（代码中为 3×3）。
# 全局平均汇聚的工作步骤如下：
# 按通道拆分：将特征图拆分为C个独立的二维矩阵，每个矩阵的尺寸为[H, W]（对应一个通道的空间特征）；
# 单通道全局平均：对每个通道的H×W个像素值计算算术平均值，得到一个标量（数值）；
# 拼接结果：将C个通道的平均值拼接成一个一维向量，维度为[N, C]。
```


```python
X = torch.rand(size = (1,1,96,96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)
```

    Sequential output shape:	 torch.Size([1, 64, 24, 24])
    Sequential output shape:	 torch.Size([1, 192, 12, 12])
    Sequential output shape:	 torch.Size([1, 480, 6, 6])
    Sequential output shape:	 torch.Size([1, 832, 3, 3])
    Sequential output shape:	 torch.Size([1, 1024])
    Linear output shape:	 torch.Size([1, 10])
    


```python

```


```python

```
